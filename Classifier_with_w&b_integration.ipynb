{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x7f9ed8860fa0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "import scanpy as sc\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "import sklearn.preprocessing\n",
    "import sklearn.model_selection\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Apple MPS on Macbook Pro\n"
     ]
    }
   ],
   "source": [
    "import platform\n",
    "if platform.platform() == 'macOS-10.16-x86_64-i386-64bit':\n",
    "    pio.renderers.default = 'notebook'\n",
    "    device = torch.device('mps')\n",
    "    print(\"Using Apple MPS on Macbook Pro\")\n",
    "    gmount = False\n",
    "    \n",
    "elif platform.platform() == 'Linux-5.10.133+-x86_64-with-Ubuntu-18.04-bionic':\n",
    "    pio.renderers.default = 'colab'\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        print(\"Using CUDA GPU on Colab\")\n",
    "        gmount = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "scdata = sc.read_h5ad(\"/Users/eamonmcandrew/Desktop/Single_cell_integration/Data/Multi-ome/GEX.h5ad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AnnData object with n_obs × n_vars = 69249 × 13431\n",
       "    obs: 'GEX_pct_counts_mt', 'GEX_n_counts', 'GEX_n_genes', 'GEX_size_factors', 'GEX_phase', 'ATAC_nCount_peaks', 'ATAC_atac_fragments', 'ATAC_reads_in_peaks_frac', 'ATAC_blacklist_fraction', 'ATAC_nucleosome_signal', 'cell_type', 'batch', 'ATAC_pseudotime_order', 'GEX_pseudotime_order', 'Samplename', 'Site', 'DonorNumber', 'Modality', 'VendorLot', 'DonorID', 'DonorAge', 'DonorBMI', 'DonorBloodType', 'DonorRace', 'Ethnicity', 'DonorGender', 'QCMeds', 'DonorSmoker'\n",
       "    var: 'feature_types', 'gene_id'\n",
       "    uns: 'ATAC_gene_activity_var_names', 'dataset_id', 'genome', 'organism'\n",
       "    obsm: 'ATAC_gene_activity', 'ATAC_lsi_full', 'ATAC_lsi_red', 'ATAC_umap', 'GEX_X_pca', 'GEX_X_umap'\n",
       "    layers: 'counts'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stratified_split(data, test_size, random_state, split_criteria):\n",
    "    \"\"\"\n",
    "    Splits the data into train and test sets stratified by the batch column\n",
    "    \"\"\"\n",
    "    train = []\n",
    "    test = []\n",
    "    for batch in data.obs[split_criteria].unique():\n",
    "        batch_data = data[data.obs[split_criteria] == batch]\n",
    "        batch_train, batch_test = sklearn.model_selection.train_test_split(batch_data, test_size=test_size, random_state=random_state)\n",
    "        batch_train, batch_test = list(batch_train.obs.index), list(batch_test.obs.index)\n",
    "        train.extend(batch_train)\n",
    "        test.extend(batch_test)\n",
    "        \n",
    "    return train, test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = stratified_split(scdata, 0.2, 9000, split_criteria='cell_type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55392, 13857)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = scdata[train]\n",
    "test_data = scdata[test]\n",
    "\n",
    "len(train_data), len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if gmount == True:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    path = '/content/drive/My Drive/Colab Notebooks/Experiments/' \n",
    "    scdata = sc.read_h5ad(\"/content/gdrive/MyDrive/scintegration/GEX.h5ad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33meamomc\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use own weights and biases account by adding the Auth token when prompted, can also use key = 'offline' to use offline\n",
    "\n",
    "import wandb\n",
    "wandb.login()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33meamomc\u001b[0m (\u001b[33mscintegration\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/eamonmcandrew/Desktop/Single_cell_integration/Github/scIntegration/wandb/run-20221205_100737-121lvzir</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/scintegration/Single%20Cell%20Omics%20integration/runs/121lvzir\" target=\"_blank\">warm-bee-309</a></strong> to <a href=\"https://wandb.ai/scintegration/Single%20Cell%20Omics%20integration\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/scintegration/Single%20Cell%20Omics%20integration/runs/121lvzir?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f9eaad54280>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project=\"Single Cell Omics integration\", entity=\"scintegration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sweep_id = wandb.sweep(sweep=sweep_configuration, project=\"project-name\")\n",
    "# sweep_configuration = {\n",
    "#     'method': 'random',\n",
    "#     'name': 'sweep',\n",
    "#     'metric': {\n",
    "#         'goal': 'maximise', \n",
    "#         'name': 'accuracy'\n",
    "# \t\t},\n",
    "#     'parameters': {\n",
    "#         'batch_size': {'values': [128, 256, 512]},\n",
    "#         'epochs': {'values': [5, 10, 15]},\n",
    "#         'lr': {'max': 0.1, 'min': 0.0001}\n",
    "#      }\n",
    "# }\n",
    "# wandb.agent(sweep_id=sweep_id, function=function_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GEX_Dataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, data, scaler=None, cat_var=None, label_encoder=None):\n",
    "        self.data = data\n",
    "        self.values = data.X.todense()\n",
    "        self.cat_var = cat_var\n",
    "\n",
    "        label_encoder_functions = {\n",
    "            \"numeric\": lambda: torch.tensor(sklearn.preprocessing.LabelEncoder().fit_transform(self.data.obs[self.cat_var]), dtype=torch.long),\n",
    "            \"range_map\": lambda: sklearn.preprocessing.LabelEncoder().fit_transform(self.data.obs[self.cat_var]).reshape(-1, 1),\n",
    "            \"one_hot\": lambda: sklearn.preprocessing.OneHotEncoder().fit_transform(sklearn.preprocessing.LabelEncoder().fit_transform(self.data.obs[self.cat_var]).reshape(-1, 1)).toarray()\n",
    "        }\n",
    "\n",
    "        if label_encoder in label_encoder_functions:\n",
    "            cat_var_data = label_encoder_functions[label_encoder]()\n",
    "            if label_encoder == \"range_map\":\n",
    "                cat_var_data = torch.tensor(sklearn.preprocessing.MinMaxScaler().fit_transform(cat_var_data), dtype=torch.float32)\n",
    "            elif label_encoder == \"one_hot\":\n",
    "                cat_var_data = torch.tensor(cat_var_data, dtype=torch.float32)\n",
    "        else:\n",
    "            cat_var_data = None\n",
    "        self.cat_var_data = cat_var_data\n",
    "\n",
    "        scaler_functions = {\n",
    "            \"Standard\": lambda: sklearn.preprocessing.StandardScaler().fit_transform(self.values),\n",
    "            \"MinMax\": lambda: sklearn.preprocessing.MinMaxScaler().fit_transform(self.values)\n",
    "        }\n",
    "\n",
    "        if scaler in scaler_functions:\n",
    "            self.scaled_values = torch.tensor(scaler_functions[scaler](), dtype=torch.float32)\n",
    "        else:\n",
    "            self.scaled_values = torch.tensor(self.values, dtype=torch.float32)\n",
    "\n",
    "    @property\n",
    "    def n_features(self):\n",
    "        return self.values.shape[1]\n",
    "\n",
    "    @property\n",
    "    def n_catagories(self):\n",
    "        return self.cat_var_data.shape[1] if self.cat_var_data is not None else 0\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.scaled_values[idx], self.cat_var_data[idx]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eamonmcandrew/opt/anaconda3/envs/scINTEGRATION/lib/python3.9/site-packages/sklearn/utils/validation.py:727: FutureWarning:\n",
      "\n",
      "np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "\n",
      "/Users/eamonmcandrew/opt/anaconda3/envs/scINTEGRATION/lib/python3.9/site-packages/sklearn/utils/validation.py:727: FutureWarning:\n",
      "\n",
      "np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "\n",
      "/Users/eamonmcandrew/opt/anaconda3/envs/scINTEGRATION/lib/python3.9/site-packages/sklearn/utils/validation.py:727: FutureWarning:\n",
      "\n",
      "np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "\n",
      "/Users/eamonmcandrew/opt/anaconda3/envs/scINTEGRATION/lib/python3.9/site-packages/sklearn/utils/validation.py:727: FutureWarning:\n",
      "\n",
      "np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "\n"
     ]
    }
   ],
   "source": [
    "GEX_Dataset_train = GEX_Dataset(train_data, scaler = \"Standard\", cat_var = \"batch\", label_encoder = \"one_hot\")\n",
    "\n",
    "GEX_Dataset_test = GEX_Dataset(test_data, scaler = \"Standard\", cat_var = \"batch\", label_encoder = \"one_hot\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-0.0823, -0.1473, -0.1277,  ..., -0.6189, -0.1985, -0.0615]),\n",
       " tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GEX_Dataset_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = GEX_Dataset_train.n_features\n",
    "output_size = GEX_Dataset_train.n_catagories\n",
    "\n",
    "\n",
    "batch_size = 256\n",
    "epochs = 30\n",
    "lr = 1e-4\n",
    "dropout = 0.2\n",
    "\n",
    "\n",
    "\n",
    "wandb.config = {\n",
    "  \"learning_rate\": lr,\n",
    "  \"epochs\": epochs,\n",
    "  \"batch_size\": batch_size,\n",
    "  \"dropout\": dropout,\n",
    "}\n",
    "\n",
    "log_interval = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(classifier, self).__init__()\n",
    "        self.cfc1 = nn.Linear(input_size, 20)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.cfc2 = nn.Linear(20, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.cfc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.cfc2(x)\n",
    "        x = F.softmax(x, dim = 1)\n",
    "        return(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "classifier(\n",
       "  (cfc1): Linear(in_features=13431, out_features=20, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       "  (cfc2): Linear(in_features=20, out_features=13, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GEX_dataloader_train = torch.utils.data.DataLoader(GEX_Dataset_train, batch_size = batch_size, shuffle = True)\n",
    "GEX_dataloader_test = torch.utils.data.DataLoader(GEX_Dataset_test, batch_size = batch_size, shuffle = True)\n",
    "model = classifier()\n",
    "model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "model.train()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(epoch, GEX_dataloader_train , model, optimizer, criterion):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(GEX_dataloader_train):\n",
    "        epoch_loss_list = []\n",
    "        epoch_accuracy_list = [] \n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        accuracy = (output.argmax(1) == target.argmax(1)).type(torch.float).mean().item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss_list.append(loss.item())\n",
    "        epoch_accuracy_list.append(accuracy)\n",
    "        wandb.log({\"Train loss\": loss.item()})\n",
    "        wandb.log({\"Train accuracy\": accuracy})\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(GEX_dataloader_train.dataset),\n",
    "                100. * batch_idx / len(GEX_dataloader_train), loss.item()))\n",
    "    epoch_loss = np.mean(epoch_loss_list)\n",
    "    epoch_accuracy = np.mean(epoch_accuracy_list)\n",
    "    wandb.log({\"Train epoch loss\": epoch_loss})\n",
    "    wandb.log({\"Train epoch accuracy\": epoch_accuracy})\n",
    "    \n",
    "    return epoch_loss, epoch_accuracy\n",
    "\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_one_epoch(epoch, GEX_Dataset_test, model, optimizer, criterion):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, target) in enumerate(GEX_dataloader_test):\n",
    "            epoch_loss_list = []\n",
    "            epoch_accuracy_list = [] \n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            accuracy = (output.argmax(1) == target.argmax(1)).type(torch.float).mean().item()\n",
    "            epoch_loss_list.append(loss.item())\n",
    "            epoch_accuracy_list.append(accuracy)\n",
    "            wandb.log({\"Test loss\": loss.item()})\n",
    "            wandb.log({\"Test accuracy\": accuracy})\n",
    "            ground_truth_class_ids = target.argmax(1).cpu().numpy()\n",
    "            predicted_class_ids = output.argmax(1).cpu().numpy()\n",
    "            wandb.log({\"conf_mat\" : wandb.plot.confusion_matrix(probs=None, y_true=ground_truth_class_ids, preds=predicted_class_ids, class_names=scdata.obs[\"batch\"].unique())})\n",
    "            if batch_idx % log_interval == 0:\n",
    "                print('Test Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    epoch, batch_idx * len(data), len(GEX_dataloader_test.dataset),\n",
    "                    100. * batch_idx / len(GEX_dataloader_test), loss.item()))\n",
    "        epoch_loss = np.mean(epoch_loss_list)\n",
    "        epoch_accuracy = np.mean(epoch_accuracy_list)\n",
    "        wandb.log({\"Test epoch loss\": epoch_loss})\n",
    "        wandb.log({\"Test epoch accuracy\": epoch_accuracy})\n",
    "        \n",
    "      \n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "    return epoch_loss, epoch_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1, epochs + 1):\n",
    "    train_epoch_loss, train_epoch_accuracy = train_one_epoch(epoch, GEX_dataloader_train, model, optimizer, criterion)\n",
    "    print(f\"Epoch training loss: {train_epoch_loss}, Epoch training accuracy: {train_epoch_accuracy}\")\n",
    "    test_epoch_loss, test_epoch_accuracy = evaluate_one_epoch(epoch, GEX_dataloader_test, model, optimizer, criterion)\n",
    "    print(f\"Epoch Eval loss: {test_epoch_loss}, Epoch Eval accuracy: {test_epoch_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('scINTEGRATION')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1d226e3599a48bcd2e3e064e4b49e64b5c23bb1e3c85e4572c7816e0051bede7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
