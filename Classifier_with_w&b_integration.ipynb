{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "import scanpy as sc\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "import sklearn.preprocessing\n",
    "import sklearn.model_selection\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "if platform.platform() == 'macOS-10.16-x86_64-i386-64bit':\n",
    "    pio.renderers.default = 'notebook'\n",
    "    device = torch.device('mps')\n",
    "    print(\"Using Apple MPS on Macbook Pro\")\n",
    "    gmount = False\n",
    "    \n",
    "elif platform.platform() == 'Linux-5.10.133+-x86_64-with-Ubuntu-18.04-bionic':\n",
    "    pio.renderers.default = 'colab'\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        print(\"Using CUDA GPU on Colab\")\n",
    "        gmount = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scdata = sc.read_h5ad(\"/Users/eamonmcandrew/Desktop/Single_cell_integration/Data/Multi-ome/GEX.h5ad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stratified_split(data, test_size, random_state, split_criteria):\n",
    "    \"\"\"\n",
    "    Splits the data into train and test sets stratified by the batch column\n",
    "    \"\"\"\n",
    "    train = []\n",
    "    test = []\n",
    "    for batch in data.obs[split_criteria].unique():\n",
    "        batch_data = data[data.obs[split_criteria] == batch]\n",
    "        batch_train, batch_test = sklearn.model_selection.train_test_split(batch_data, test_size=test_size, random_state=random_state)\n",
    "        batch_train, batch_test = list(batch_train.obs.index), list(batch_test.obs.index)\n",
    "        train.extend(batch_train)\n",
    "        test.extend(batch_test)\n",
    "        \n",
    "    return train, test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = stratified_split(scdata, 0.2, 9000, split_criteria='cell_type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = scdata[train]\n",
    "test_data = scdata[test]\n",
    "\n",
    "len(train_data), len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if gmount == True:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    path = '/content/drive/My Drive/Colab Notebooks/Experiments/' \n",
    "    scdata = sc.read_h5ad(\"/content/gdrive/MyDrive/scintegration/GEX.h5ad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use own weights and biases account by adding the Auth token when prompted, can also use key = 'offline' to use offline\n",
    "\n",
    "import wandb\n",
    "wandb.login()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(project=\"Single Cell Omics integration\", entity=\"scintegration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sweep_id = wandb.sweep(sweep=sweep_configuration, project=\"project-name\")\n",
    "# sweep_configuration = {\n",
    "#     'method': 'random',\n",
    "#     'name': 'sweep',\n",
    "#     'metric': {\n",
    "#         'goal': 'maximise', \n",
    "#         'name': 'accuracy'\n",
    "# \t\t},\n",
    "#     'parameters': {\n",
    "#         'batch_size': {'values': [128, 256, 512]},\n",
    "#         'epochs': {'values': [5, 10, 15]},\n",
    "#         'lr': {'max': 0.1, 'min': 0.0001}\n",
    "#      }\n",
    "# }\n",
    "# wandb.agent(sweep_id=sweep_id, function=function_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GEX_Dataset(torch.utils.data.Dataset):\n",
    "    \n",
    "      def __init__(self, data,  scaler = None, cat_var = None, label_encoder =None):\n",
    "          \n",
    "            self.data = data\n",
    "            \n",
    "            # we need to work with the dense matrix\n",
    "            self.values = data.X.todense()\n",
    "            \n",
    "            self.cat_var = cat_var\n",
    "            \n",
    "            if label_encoder == \"numeric\":\n",
    "            # numerically encode the labels\n",
    "              cat_var_data =  torch.tensor(sklearn.preprocessing.LabelEncoder().fit_transform(self.data.obs[self.cat_var]), dtype = torch.long)\n",
    "            \n",
    "            elif label_encoder == \"range_map\":\n",
    "              cat_var_data =  sklearn.preprocessing.LabelEncoder().fit_transform(self.data.obs[self.cat_var])\n",
    "              cat_var_data = cat_var_data.reshape(-1, 1) \n",
    "              cat_var_data = torch.tensor(sklearn.preprocessing.MinMaxScaler().fit_transform(cat_var_data), dtype = torch.float32)\n",
    "\n",
    "            elif label_encoder == \"one_hot\":\n",
    "              cat_var_data =  torch.tensor(sklearn.preprocessing.LabelEncoder().fit_transform(self.data.obs[self.cat_var]))\n",
    "              cat_var_data = cat_var_data.reshape(-1, 1)\n",
    "              cat_var_data = sklearn.preprocessing.OneHotEncoder().fit_transform(cat_var_data).toarray()\n",
    "              cat_var_data = torch.tensor(cat_var_data, dtype=torch.float32)\n",
    "              \n",
    "\n",
    "\n",
    "\n",
    "            self.cat_var_data = torch.tensor(cat_var_data)\n",
    "            \n",
    "            # scale the data according to user inpt to scaler argument\n",
    "            if scaler == \"Standard\":\n",
    "                self.scaled_values = torch.tensor(sklearn.preprocessing.StandardScaler().fit_transform(self.values), dtype = torch.float32)\n",
    "            elif scaler == \"MinMax\":\n",
    "                self.scaled_values = torch.tensor(sklearn.preprocessing.MinMaxScaler().fit_transform(self.values),  dtype = torch.float32)\n",
    "            else:\n",
    "                self.scaled_values = torch.tensor(self.values, dtype = torch.float32)\n",
    "                \n",
    "    #   return the number of genes when called \n",
    "             \n",
    "      @property\n",
    "      def n_features(self):\n",
    "          return self.values.shape[1]\n",
    "\n",
    "      @property\n",
    "      def n_catagories(self):\n",
    "          return self.cat_var_data.shape[1]\n",
    "        \n",
    "      \n",
    "          \n",
    "          \n",
    "    #  A dataset class needs the following two methods to work with the dataloader class     \n",
    "          \n",
    "    #   return the number of cells when called\n",
    "      def __len__(self):\n",
    "          return len(self.data)\n",
    "    \n",
    "    #  return an individual cell and its label when called\n",
    "      def __getitem__(self, idx):\n",
    "           return self.scaled_values[idx], self.cat_var_data[idx]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GEX_Dataset_train = GEX_Dataset(train_data, scaler = \"Standard\", cat_var = \"batch\", label_encoder = \"one_hot\")\n",
    "\n",
    "GEX_Dataset_test = GEX_Dataset(test_data, scaler = \"Standard\", cat_var = \"batch\", label_encoder = \"one_hot\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = GEX_Dataset_train.n_features\n",
    "output_size = GEX_Dataset_train.n_catagories\n",
    "\n",
    "\n",
    "batch_size = 256\n",
    "epochs = 30\n",
    "lr = 1e-4\n",
    "dropout = 0.2\n",
    "\n",
    "\n",
    "\n",
    "wandb.config = {\n",
    "  \"learning_rate\": lr,\n",
    "  \"epochs\": epochs,\n",
    "  \"batch_size\": batch_size,\n",
    "  \"dropout\": dropout,\n",
    "}\n",
    "\n",
    "log_interval = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(classifier, self).__init__()\n",
    "        self.cfc1 = nn.Linear(input_size, 20)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.cfc2 = nn.Linear(20, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.cfc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.cfc2(x)\n",
    "        x = F.softmax(x, dim = 1)\n",
    "        return(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GEX_dataloader_train = torch.utils.data.DataLoader(GEX_Dataset_train, batch_size = batch_size, shuffle = True)\n",
    "GEX_dataloader_test = torch.utils.data.DataLoader(GEX_Dataset_test, batch_size = batch_size, shuffle = True)\n",
    "model = classifier()\n",
    "model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "model.train()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(epoch, GEX_dataloader_train , model, optimizer, criterion):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(GEX_dataloader_train):\n",
    "        epoch_loss_list = []\n",
    "        epoch_accuracy_list = [] \n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        accuracy = (output.argmax(1) == target.argmax(1)).type(torch.float).mean().item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss_list.append(loss.item())\n",
    "        epoch_accuracy_list.append(accuracy)\n",
    "        wandb.log({\"Train loss\": loss.item()})\n",
    "        wandb.log({\"Train accuracy\": accuracy})\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(GEX_dataloader_train.dataset),\n",
    "                100. * batch_idx / len(GEX_dataloader_train), loss.item()))\n",
    "    epoch_loss = np.mean(epoch_loss_list)\n",
    "    epoch_accuracy = np.mean(epoch_accuracy_list)\n",
    "    wandb.log({\"Train epoch loss\": epoch_loss})\n",
    "    wandb.log({\"Train epoch accuracy\": epoch_accuracy})\n",
    "    \n",
    "    return epoch_loss, epoch_accuracy\n",
    "\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_one_epoch(epoch, GEX_Dataset_test, model, optimizer, criterion):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, target) in enumerate(GEX_dataloader_test):\n",
    "            epoch_loss_list = []\n",
    "            epoch_accuracy_list = [] \n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            accuracy = (output.argmax(1) == target.argmax(1)).type(torch.float).mean().item()\n",
    "            epoch_loss_list.append(loss.item())\n",
    "            epoch_accuracy_list.append(accuracy)\n",
    "            wandb.log({\"Test loss\": loss.item()})\n",
    "            wandb.log({\"Test accuracy\": accuracy})\n",
    "            ground_truth_class_ids = target.argmax(1).cpu().numpy()\n",
    "            predicted_class_ids = output.argmax(1).cpu().numpy()\n",
    "            wandb.log({\"conf_mat\" : wandb.plot.confusion_matrix(probs=None, y_true=ground_truth_class_ids, preds=predicted_class_ids, class_names=scdata.obs[\"batch\"].unique())})\n",
    "            if batch_idx % log_interval == 0:\n",
    "                print('Test Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    epoch, batch_idx * len(data), len(GEX_dataloader_test.dataset),\n",
    "                    100. * batch_idx / len(GEX_dataloader_test), loss.item()))\n",
    "        epoch_loss = np.mean(epoch_loss_list)\n",
    "        epoch_accuracy = np.mean(epoch_accuracy_list)\n",
    "        wandb.log({\"Test epoch loss\": epoch_loss})\n",
    "        wandb.log({\"Test epoch accuracy\": epoch_accuracy})\n",
    "        \n",
    "      \n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "    return epoch_loss, epoch_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1, epochs + 1):\n",
    "    train_epoch_loss, train_epoch_accuracy = train_one_epoch(epoch, GEX_dataloader_train, model, optimizer, criterion)\n",
    "    print(f\"Epoch training loss: {train_epoch_loss}, Epoch training accuracy: {train_epoch_accuracy}\")\n",
    "    test_epoch_loss, test_epoch_accuracy = evaluate_one_epoch(epoch, GEX_dataloader_test, model, optimizer, criterion)\n",
    "    print(f\"Epoch Eval loss: {test_epoch_loss}, Epoch Eval accuracy: {test_epoch_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('scINTEGRATION')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1d226e3599a48bcd2e3e064e4b49e64b5c23bb1e3c85e4572c7816e0051bede7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
