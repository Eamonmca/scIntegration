{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "yKRnz3qrnNXo",
        "outputId": "e9c681e5-22b7-4b0f-cf15-a137e0bc4e22"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Usage:   \n",
            "  /usr/bin/python3 -m pip uninstall [options] <package> ...\n",
            "  /usr/bin/python3 -m pip uninstall [options] -r <requirements file> ...\n",
            "\n",
            "no such option: -Y\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: matplotlib==3.1.3 in /usr/local/lib/python3.8/dist-packages (3.1.3)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib==3.1.3) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib==3.1.3) (1.4.4)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib==3.1.3) (0.11.0)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.8/dist-packages (from matplotlib==3.1.3) (1.21.6)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib==3.1.3) (3.0.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.1->matplotlib==3.1.3) (1.15.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.8/dist-packages (0.13.5)\n",
            "Requirement already satisfied: protobuf!=4.0.*,!=4.21.0,<5,>=3.12.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (3.19.6)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (1.15.0)\n",
            "Requirement already satisfied: shortuuid>=0.5.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (1.0.11)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (2.3)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from wandb) (57.4.0)\n",
            "Requirement already satisfied: GitPython>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (3.1.29)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.8/dist-packages (from wandb) (6.0)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (1.9.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.8/dist-packages (from wandb) (1.3.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (2.23.0)\n",
            "Requirement already satisfied: pathtools in /usr/local/lib/python3.8/dist-packages (from wandb) (0.1.2)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.8/dist-packages (from GitPython>=1.0.0->wandb) (4.0.10)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.8/dist-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (5.0.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.0.0->wandb) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: scanpy in /usr/local/lib/python3.8/dist-packages (1.9.1)\n",
            "Collecting matplotlib>=3.4\n",
            "  Using cached matplotlib-3.6.2-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (9.4 MB)\n",
            "Requirement already satisfied: statsmodels>=0.10.0rc2 in /usr/local/lib/python3.8/dist-packages (from scanpy) (0.12.2)\n",
            "Requirement already satisfied: pandas>=1.0 in /usr/local/lib/python3.8/dist-packages (from scanpy) (1.3.5)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.8/dist-packages (from scanpy) (0.11.2)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.8/dist-packages (from scanpy) (1.21.6)\n",
            "Requirement already satisfied: umap-learn>=0.3.10 in /usr/local/lib/python3.8/dist-packages (from scanpy) (0.5.3)\n",
            "Requirement already satisfied: h5py>=3 in /usr/local/lib/python3.8/dist-packages (from scanpy) (3.1.0)\n",
            "Requirement already satisfied: patsy in /usr/local/lib/python3.8/dist-packages (from scanpy) (0.5.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from scanpy) (4.64.1)\n",
            "Requirement already satisfied: networkx>=2.3 in /usr/local/lib/python3.8/dist-packages (from scanpy) (2.6.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from scanpy) (1.2.0)\n",
            "Requirement already satisfied: anndata>=0.7.4 in /usr/local/lib/python3.8/dist-packages (from scanpy) (0.8.0)\n",
            "Requirement already satisfied: session-info in /usr/local/lib/python3.8/dist-packages (from scanpy) (1.0.0)\n",
            "Requirement already satisfied: natsort in /usr/local/lib/python3.8/dist-packages (from scanpy) (5.5.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from scanpy) (21.3)\n",
            "Requirement already satisfied: numba>=0.41.0 in /usr/local/lib/python3.8/dist-packages (from scanpy) (0.56.4)\n",
            "Requirement already satisfied: scipy>=1.4 in /usr/local/lib/python3.8/dist-packages (from scanpy) (1.7.3)\n",
            "Requirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.8/dist-packages (from scanpy) (1.0.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.4->scanpy) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.4->scanpy) (4.38.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.4->scanpy) (1.4.4)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.4->scanpy) (1.0.6)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.4->scanpy) (2.8.2)\n",
            "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.4->scanpy) (3.0.9)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.4->scanpy) (7.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from numba>=0.41.0->scanpy) (57.4.0)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.8/dist-packages (from numba>=0.41.0->scanpy) (0.39.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.8/dist-packages (from numba>=0.41.0->scanpy) (4.13.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=1.0->scanpy) (2022.6)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7->matplotlib>=3.4->scanpy) (1.15.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.22->scanpy) (3.1.0)\n",
            "Requirement already satisfied: pynndescent>=0.5 in /usr/local/lib/python3.8/dist-packages (from umap-learn>=0.3.10->scanpy) (0.5.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata->numba>=0.41.0->scanpy) (3.10.0)\n",
            "Requirement already satisfied: stdlib-list in /usr/local/lib/python3.8/dist-packages (from session-info->scanpy) (0.8.0)\n",
            "Installing collected packages: matplotlib\n",
            "  Attempting uninstall: matplotlib\n",
            "    Found existing installation: matplotlib 3.1.3\n",
            "    Uninstalling matplotlib-3.1.3:\n",
            "      Successfully uninstalled matplotlib-3.1.3\n",
            "Successfully installed matplotlib-3.6.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "matplotlib",
                  "mpl_toolkits"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        " %matplotlib inline\n",
        "!python -m pip uninstall matplotlib -Y\n",
        "!pip install matplotlib==3.1.3 \n",
        "!pip install wandb\n",
        "!pip install scanpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "andwqcpJmgZO",
        "outputId": "46620d4a-3819-4440-da9a-8823d332a736"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x7f70ed3a88b0>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import torch\n",
        "from torch import nn, optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import SubsetRandomSampler\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import itertools\n",
        "import scanpy as sc\n",
        "import plotly.express as px\n",
        "import plotly.io as pio\n",
        "import sklearn.preprocessing\n",
        "import sklearn.model_selection\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "torch.autograd.set_detect_anomaly(True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "uAEfebOCnMkw"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "fyeiaHCwmgZQ"
      },
      "outputs": [],
      "source": [
        "import platform\n",
        "\n",
        "def get_device_and_gmount():\n",
        "    # Get the operating system and version\n",
        "    os = platform.system()\n",
        "    version = platform.release()\n",
        "\n",
        "    # Get the machine's architecture\n",
        "    arch = platform.machine()\n",
        "\n",
        "    # Set the default renderer based on the operating system\n",
        "    if os == 'Darwin':\n",
        "        pio.renderers.default = 'notebook'\n",
        "        print(\"Using Apple MPS on Macbook Pro\")\n",
        "    \n",
        "    elif os == 'Linux':\n",
        "        pio.renderers.default = 'colab'\n",
        "        print(\"Using Colab on Linux\")\n",
        "\n",
        "    # Set the device based on the machine's architecture\n",
        "    if arch == 'x86_64':\n",
        "        device = torch.device('mps') if os == 'Darwin' else torch.device('cuda')\n",
        "        gmount = True if os == 'Linux' else False\n",
        "    else:\n",
        "        device = torch.device('cpu')\n",
        "        gmount = False\n",
        "\n",
        "    print(\"Using device:\", device)\n",
        "    \n",
        "    return device, gmount\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Jn5HT1RmgZR",
        "outputId": "64877760-4c3f-4b85-fdea-38281eebba9d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Colab on Linux\n",
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "device, gmount = get_device_and_gmount()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pFszDCKQmgZR",
        "outputId": "f2f855f1-eaac-4bb9-d3cc-cbc038e203e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "if gmount:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    scdata = sc.read_h5ad(\"/content/drive/MyDrive/scintegration/GEX.h5ad\")\n",
        "else:\n",
        "    scdata = sc.read_h5ad(\"/Users/eamonmcandrew/Desktop/Single_cell_integration/Data/Multi-ome/GEX.h5ad\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "l8nIHsqGmgZR"
      },
      "outputs": [],
      "source": [
        "def stratified_split(data, test_size, random_state, split_criteria):\n",
        "    \"\"\"\n",
        "    Splits the data into train and test sets stratified by the batch column\n",
        "    \"\"\"\n",
        "    train = []\n",
        "    test = []\n",
        "    for batch in data.obs[split_criteria].unique():\n",
        "        batch_data = data[data.obs[split_criteria] == batch]\n",
        "        batch_train, batch_test = sklearn.model_selection.train_test_split(batch_data, test_size=test_size, random_state=random_state)\n",
        "        batch_train, batch_test = list(batch_train.obs.index), list(batch_test.obs.index)\n",
        "        train.extend(batch_train)\n",
        "        test.extend(batch_test)\n",
        "        \n",
        "    return train, test\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "re9X5gmLmgZS",
        "outputId": "7a0fd4b4-0458-4817-d7c1-02f141aa8f0c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "if gmount == True:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    scdata = sc.read_h5ad(\"/content/drive/MyDrive/scintegration/GEX.h5ad\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cT6PVLo8mgZS",
        "outputId": "5a9a3328-2749-4c05-9714-c7283eae082d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:wandb.jupyter:Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33meamomc\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "import wandb\n",
        "wandb.login()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "gcP6QkdbmgZS"
      },
      "outputs": [],
      "source": [
        "class GEX_Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, data, scaler=None, cat_var=None, label_encoder=None):\n",
        "        self.data = data\n",
        "        self.values = np.asarray(data.X.todense())\n",
        "        self.cat_var = cat_var\n",
        "\n",
        "        label_encoder_functions = {\n",
        "            \"numeric\": lambda: torch.tensor(sklearn.preprocessing.LabelEncoder().fit_transform(self.data.obs[self.cat_var]), dtype=torch.long),\n",
        "            \"range_map\": lambda: sklearn.preprocessing.LabelEncoder().fit_transform(self.data.obs[self.cat_var]).reshape(-1, 1),\n",
        "            \"one_hot\": lambda: sklearn.preprocessing.OneHotEncoder().fit_transform(sklearn.preprocessing.LabelEncoder().fit_transform(self.data.obs[self.cat_var]).reshape(-1, 1)).toarray()\n",
        "        }\n",
        "\n",
        "        if label_encoder in label_encoder_functions:\n",
        "            cat_var_data = label_encoder_functions[label_encoder]()\n",
        "            if label_encoder == \"range_map\":\n",
        "                cat_var_data = torch.tensor(sklearn.preprocessing.MinMaxScaler().fit_transform(cat_var_data), dtype=torch.float32)\n",
        "            elif label_encoder == \"one_hot\":\n",
        "                cat_var_data = torch.tensor(cat_var_data, dtype=torch.float32)\n",
        "        else:\n",
        "            cat_var_data = None\n",
        "        self.cat_var_data = cat_var_data\n",
        "\n",
        "        scaler_functions = {\n",
        "            \"Standard\": lambda: sklearn.preprocessing.StandardScaler().fit_transform(self.values),\n",
        "            \"MinMax\": lambda: sklearn.preprocessing.MinMaxScaler().fit_transform(self.values)\n",
        "        }\n",
        "\n",
        "        if scaler in scaler_functions:\n",
        "            self.scaled_values = torch.tensor(scaler_functions[scaler](), dtype=torch.float32)\n",
        "        else:\n",
        "            self.scaled_values = torch.tensor(self.values, dtype=torch.float32)\n",
        "\n",
        "    @property\n",
        "    def n_features(self):\n",
        "        return self.values.shape[1]\n",
        "\n",
        "    @property\n",
        "    def n_catagories(self):\n",
        "        return self.cat_var_data.shape[1] if self.cat_var_data is not None else 0\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.scaled_values[idx], self.cat_var_data[idx]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "cANaD_q4-XD8"
      },
      "outputs": [],
      "source": [
        "def stratified_split(data, test_size, random_state, split_criteria):\n",
        "    \"\"\"\n",
        "    Splits the data into train and test sets stratified by the batch column\n",
        "    \"\"\"\n",
        "    train = []\n",
        "    test = []\n",
        "    for batch in data.obs[split_criteria].unique():\n",
        "        batch_data = data[data.obs[split_criteria] == batch]\n",
        "        batch_train, batch_test = sklearn.model_selection.train_test_split(batch_data, test_size=test_size, random_state=random_state)\n",
        "        batch_train, batch_test = list(batch_train.obs.index), list(batch_test.obs.index)\n",
        "        train.extend(batch_train)\n",
        "        test.extend(batch_test)\n",
        "        \n",
        "    return train, test\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "fOkl1P4wmgZT"
      },
      "outputs": [],
      "source": [
        "class Generator(nn.Module):\n",
        "  def __init__(self, input_size, dropout, hidden_sizes, output_size, use_batch_norm):\n",
        "    super().__init__()\n",
        "\n",
        "    self.input_size = input_size\n",
        "    self.dropout = dropout\n",
        "    self.hidden_sizes = hidden_sizes\n",
        "    self.output_size = output_size\n",
        "    self.use_batch_norm = use_batch_norm\n",
        "\n",
        "    # create a list of layers\n",
        "    layers = []\n",
        "\n",
        "    # input layer\n",
        "    layers.append(nn.Linear(self.input_size, self.hidden_sizes[0]))\n",
        "    if self.use_batch_norm:\n",
        "      layers.append(nn.BatchNorm1d(self.hidden_sizes[0]))\n",
        "    layers.append(nn.ReLU())\n",
        "    if self.dropout > 0:\n",
        "      layers.append(nn.Dropout(p=self.dropout))\n",
        "\n",
        "    # hidden layers\n",
        "    for i in range(1, len(self.hidden_sizes)):\n",
        "      layers.append(nn.Linear(self.hidden_sizes[i-1], self.hidden_sizes[i]))\n",
        "      if self.use_batch_norm:\n",
        "        layers.append(nn.BatchNorm1d(self.hidden_sizes[i]))\n",
        "      layers.append(nn.ReLU())\n",
        "      if self.dropout > 0:\n",
        "        layers.append(nn.Dropout(p=self.dropout))\n",
        "\n",
        "    # output layer\n",
        "    layers.append(nn.Linear(self.hidden_sizes[-1], self.output_size))\n",
        "    layers.append(nn.Tanh())\n",
        "\n",
        "    # create the model using Sequential\n",
        "    self.model = nn.Sequential(*layers)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.model(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "yD-8i06-mgZT"
      },
      "outputs": [],
      "source": [
        "class Discriminator(nn.Module):\n",
        "  def __init__(self, input_size, dropout, hidden_sizes, output_size, use_batch_norm):\n",
        "    super().__init__()\n",
        "\n",
        "    self.input_size = input_size\n",
        "    self.dropout = dropout\n",
        "    self.hidden_sizes = hidden_sizes\n",
        "    self.output_size = output_size\n",
        "    self.use_batch_norm = use_batch_norm\n",
        "\n",
        "    # create a list of layers\n",
        "    layers = []\n",
        "\n",
        "    # input layer\n",
        "    layers.append(nn.Linear(self.input_size, self.hidden_sizes[0]))\n",
        "    layers.append(nn.LeakyReLU(0.2))\n",
        "    if self.dropout > 0:\n",
        "      layers.append(nn.Dropout(p=self.dropout))\n",
        "\n",
        "    # hidden layers\n",
        "    for i in range(1, len(self.hidden_sizes)):\n",
        "      layers.append(nn.Linear(self.hidden_sizes[i-1], self.hidden_sizes[i]))\n",
        "      if self.use_batch_norm:\n",
        "        layers.append(nn.BatchNorm1d(self.hidden_sizes[i]))\n",
        "      layers.append(nn.LeakyReLU(0.2))\n",
        "      if self.dropout > 0:\n",
        "        layers.append(nn.Dropout(p=self.dropout))\n",
        "\n",
        "    # output layer\n",
        "    layers.append(nn.Linear(self.hidden_sizes[-1], self.output_size))\n",
        "\n",
        "    # create the model using Sequential\n",
        "    self.model = nn.Sequential(*layers)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.model(x)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "DDKHovIomgZT"
      },
      "outputs": [],
      "source": [
        "# Define the GAN model\n",
        "class GAN(nn.Module):\n",
        "    def __init__(self, generator, discriminator):\n",
        "        super().__init__()\n",
        "        self.generator = generator\n",
        "        self.discriminator = discriminator\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.discriminator(self.generator(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "XSIDCxDX0res"
      },
      "outputs": [],
      "source": [
        "def get_noise(n_samples, z_dim, device=device):\n",
        "    '''\n",
        "    Function for creating noise vectors: Given the dimensions (n_samples, z_dim),\n",
        "    creates a tensor of that shape filled with random numbers from the normal distribution.\n",
        "    Parameters:\n",
        "        n_samples: the number of samples to generate, a scalar\n",
        "        z_dim: the dimension of the noise vector, a scalar\n",
        "        device: the device type\n",
        "    '''\n",
        "    return torch.randn(n_samples,z_dim,device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "HoK2bQef5Tkg"
      },
      "outputs": [],
      "source": [
        "def generate_fake_data_and_combine(original_data, generator, z_dim, batch_size):\n",
        "    generator.eval()\n",
        "    noise = get_noise(batch_size, z_dim, device=device)\n",
        "    fake_data = generator(noise)\n",
        "    fake_adata = sc.AnnData(fake_data.detach().cpu().numpy())\n",
        "\n",
        "    # add an index column to the fake data, starting at the number of real cells\n",
        "    fake_adata.var.index = original_data.var.index\n",
        "    adata = sc.AnnData.concatenate(original_data, fake_adata)\n",
        "    # add a \"group\" column to the observations DataFrame,\n",
        "    # with the value \"real\" for the real cells and \"fake\" for the fake cells\n",
        "    adata.obs['group'] = ['real' if i < len(original_data.obs) else 'fake' for i in range(len(adata.obs))]\n",
        "\n",
        "    # compute the UMAP of the combined data\n",
        "    sc.pp.neighbors(adata, n_neighbors=10)\n",
        "    sc.tl.umap(adata)\n",
        "\n",
        "    return adata\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "EuXO3mOtBQmG"
      },
      "outputs": [],
      "source": [
        "def plot_umap_by_group(adata, color):\n",
        "    plot = sc.pl.umap(adata, color=color, legend_loc='on data', show=False)\n",
        "    return plot "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "hfcdWwrz5hZo"
      },
      "outputs": [],
      "source": [
        "def train_gan_one_epoch(epoch, GEX_dataloader_train, gan, g_opt, d_opt, criterion, z_dim):\n",
        "    gan.generator.to(device)\n",
        "    gan.discriminator.to(device)\n",
        "    gan.train()\n",
        "    total_loss_discriminator = 0\n",
        "    total_loss_generator = 0\n",
        "    for iteration, (data, _) in enumerate(GEX_dataloader_train):\n",
        "        data = data.to(device)\n",
        "        # Generate fake data\n",
        "        noise = get_noise(data.shape[0], z_dim, device=device)\n",
        "        fake_data = gan.generator(noise)\n",
        "\n",
        "        fake_data = fake_data.to(device)\n",
        "\n",
        "        # Train the discriminator\n",
        "        d_opt.zero_grad()\n",
        "        pred_real = gan.discriminator(data)\n",
        "        pred_fake = gan.discriminator(fake_data.detach())\n",
        "        loss_real = criterion(pred_real, torch.ones_like(pred_real))\n",
        "        loss_fake = criterion(pred_fake, torch.zeros_like(pred_fake))\n",
        "        loss_discriminator = (loss_real + loss_fake) / 2\n",
        "        loss_discriminator.backward(retain_graph=True)\n",
        "        wandb.log({\"loss_discriminator\": loss_discriminator})\n",
        "        d_opt.step()\n",
        "\n",
        "        # Train the generator\n",
        "        g_opt.zero_grad()\n",
        "        noise = get_noise(data.shape[0], z_dim, device=device)\n",
        "        pred_fake = gan.discriminator(fake_data)\n",
        "        loss_generator = criterion(pred_fake, torch.ones_like(pred_fake))\n",
        "        loss_generator.backward()\n",
        "        wandb.log({\"loss_generator\": loss_generator})\n",
        "        g_opt.step()\n",
        "\n",
        "        total_loss_discriminator += loss_discriminator.item()\n",
        "        total_loss_generator += loss_generator.item()\n",
        "\n",
        "    num_iterations = len(GEX_dataloader_train)\n",
        "    mean_loss_discriminator = total_loss_discriminator / num_iterations\n",
        "    mean_loss_generator = total_loss_generator / num_iterations\n",
        "\n",
        "    return mean_loss_discriminator, mean_loss_generator\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "YzQ5OvBJGCWj"
      },
      "outputs": [],
      "source": [
        "config = wandb.config = {\n",
        "    \"lr\": 0.001,\n",
        "    \"batch_size\": 256,\n",
        "    \"epochs\": 1000,\n",
        "    \"random_seed\": 42,\n",
        "    \"dropout_G\": 0,\n",
        "    \"dropout_D\": 0.2,\n",
        "    \"split_criteria\": \"cell_type\",\n",
        "    \"eval_size_percentage\": 0.1,\n",
        "    \"hidden_sizes_G\": [32, 64, 128],\n",
        "    \"hidden_sizes_D\": [32, 64, 128],\n",
        "    \"use_batch_norm_G\": True,\n",
        "    \"use_batch_norm_D\": True,\n",
        "    \"z_dim\": 100,\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "AhfFLgLZz0iY"
      },
      "outputs": [],
      "source": [
        "def train_func(config):\n",
        "    \n",
        "    if config is not None:\n",
        "        run = wandb.init(project=\"Single Cell Omics integration\", entity=\"scintegration\", config=config)\n",
        "\n",
        "    else:\n",
        "      run = wandb.init(project=\"Single Cell Omics integration\", entity=\"scintegration\", config = config)\n",
        "    \n",
        "\n",
        "    # Load the learning rate, batch size, epochs, random seed, dropout, and hidden size from the wandb configuration\n",
        "\n",
        "    lr = wandb.config.lr\n",
        "    batch_size = wandb.config.batch_size\n",
        "    epochs = wandb.config.epochs\n",
        "    random_seed = wandb.config.random_seed\n",
        "    dropout_G = wandb.config.dropout_G\n",
        "    dropout_D = wandb.config.dropout_D\n",
        "    split_criteria = wandb.config.split_criteria\n",
        "    eval_size_percentage = wandb.config.eval_size_percentage\n",
        "    hidden_sizes_G = wandb.config.hidden_sizes_G\n",
        "    hidden_sizes_D = wandb.config.hidden_sizes_D\n",
        "    use_batch_norm_G = wandb.config.use_batch_norm_G\n",
        "    use_batch_norm_D = wandb.config.use_batch_norm_D\n",
        "    z_dim = wandb.config.z_dim\n",
        "\n",
        "\n",
        "    train, test = stratified_split(scdata, 0.2, wandb.config.random_seed, split_criteria=split_criteria)\n",
        "    train_data = scdata[train]\n",
        "    test_data = scdata[test]\n",
        "\n",
        "\n",
        "    GEX_Dataset_train = GEX_Dataset(train_data, scaler=\"Standard\", cat_var=\"batch\", label_encoder=\"one_hot\")\n",
        "    GEX_dataloader_train = torch.utils.data.DataLoader(GEX_Dataset_train, batch_size = batch_size, shuffle = True)\n",
        "\n",
        "    GEX_Dataset_test = GEX_Dataset(test_data, scaler=\"Standard\", cat_var=\"batch\", label_encoder=\"one_hot\")\n",
        "\n",
        "    output_size = GEX_Dataset_train.n_features\n",
        "    target_size = GEX_Dataset_train.n_features\n",
        "\n",
        "    # Create instance of the generator and discriminator networks\n",
        "    generator = Generator(output_size=target_size, input_size=z_dim, hidden_sizes=hidden_sizes_G, dropout=dropout_G, use_batch_norm=use_batch_norm_G)\n",
        "    generator = generator.to(device)\n",
        "\n",
        "\n",
        "    discriminator = Discriminator(input_size=target_size, output_size=1, hidden_sizes=hidden_sizes_D, use_batch_norm = use_batch_norm_D, dropout=dropout_D)\n",
        "    discriminator = discriminator.to(device)\n",
        "\n",
        "\n",
        "    gan = GAN(generator, discriminator)\n",
        "    gan = gan.to(device)\n",
        "    print(gan)\n",
        "\n",
        "    loss_fn = nn.BCEWithLogitsLoss()\n",
        "    d_opt = torch.optim.Adam(gan.discriminator.parameters(), lr=lr)\n",
        "    g_opt = torch.optim.Adam(gan.generator.parameters(), lr=lr)\n",
        "\n",
        "\n",
        "    test_data =  sc.AnnData(np.array(np.array(GEX_Dataset_test[:])[0]))\n",
        "    test_data_ = test_data\n",
        "    sc.pp.neighbors(test_data, n_neighbors=10)\n",
        "    sc.tl.umap(test_data)\n",
        "    test_UMAP = sc.pl.umap(test_data, show=False)\n",
        "    wandb.log({\"Test UMAP baseline\": wandb.Image(test_UMAP)})\n",
        "\n",
        "    # Create instance of the GAN model\n",
        "    gan.train()\n",
        "    for epoch in range(1,epochs):\n",
        "      wandb.log({\"epoch\": epoch})\n",
        "      D_loss_train, G_loss_train = train_gan_one_epoch(epoch, GEX_dataloader_train, gan, g_opt, d_opt, loss_fn, z_dim)\n",
        "      wandb.log({\"D_loss_train\": D_loss_train, \"G_loss_train\": G_loss_train})\n",
        "      if epoch % 25 == 0:\n",
        "        umap_data = generate_fake_data_and_combine(test_data, generator, z_dim, batch_size)\n",
        "        umap = plot_umap_by_group(umap_data, color = \"group\")\n",
        "        wandb.log({\"Test UMAP\": wandb.Image(umap)})\n",
        "\n",
        "      \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SCnHz64fuJkZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a8da3036-d336-428c-b88a-e001a6f7a973"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33meamomc\u001b[0m (\u001b[33mscintegration\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.13.5"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20221205_160607-2s4gtftn</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/scintegration/Single%20Cell%20Omics%20integration/runs/2s4gtftn\" target=\"_blank\">cosmic-oath-344</a></strong> to <a href=\"https://wandb.ai/scintegration/Single%20Cell%20Omics%20integration\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GAN(\n",
            "  (generator): Generator(\n",
            "    (model): Sequential(\n",
            "      (0): Linear(in_features=100, out_features=32, bias=True)\n",
            "      (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): ReLU()\n",
            "      (3): Linear(in_features=32, out_features=64, bias=True)\n",
            "      (4): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (5): ReLU()\n",
            "      (6): Linear(in_features=64, out_features=128, bias=True)\n",
            "      (7): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (8): ReLU()\n",
            "      (9): Linear(in_features=128, out_features=13431, bias=True)\n",
            "      (10): Tanh()\n",
            "    )\n",
            "  )\n",
            "  (discriminator): Discriminator(\n",
            "    (model): Sequential(\n",
            "      (0): Linear(in_features=13431, out_features=32, bias=True)\n",
            "      (1): LeakyReLU(negative_slope=0.2)\n",
            "      (2): Dropout(p=0.2, inplace=False)\n",
            "      (3): Linear(in_features=32, out_features=64, bias=True)\n",
            "      (4): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (5): LeakyReLU(negative_slope=0.2)\n",
            "      (6): Dropout(p=0.2, inplace=False)\n",
            "      (7): Linear(in_features=64, out_features=128, bias=True)\n",
            "      (8): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (9): LeakyReLU(negative_slope=0.2)\n",
            "      (10): Dropout(p=0.2, inplace=False)\n",
            "      (11): Linear(in_features=128, out_features=1, bias=True)\n",
            "    )\n",
            "  )\n",
            ")\n",
            "WARNING: You’re trying to run this on 13431 dimensions of `.X`, if you really want this, set `use_rep='X'`.\n",
            "         Falling back to preprocessing with `sc.pp.pca` and default params.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-19-a13e7a5d9acd>:58: FutureWarning:\n",
            "\n",
            "The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
            "\n",
            "<ipython-input-19-a13e7a5d9acd>:58: VisibleDeprecationWarning:\n",
            "\n",
            "Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "\n",
            "/usr/local/lib/python3.8/dist-packages/scanpy/plotting/_tools/scatterplots.py:392: UserWarning:\n",
            "\n",
            "No data for colormapping provided via 'c'. Parameters 'cmap', 'norm' will be ignored\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: You’re trying to run this on 13431 dimensions of `.X`, if you really want this, set `use_rep='X'`.\n",
            "         Falling back to preprocessing with `sc.pp.pca` and default params.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/anndata/_core/anndata.py:1785: FutureWarning:\n",
            "\n",
            "X.dtype being converted to np.float32 from float64. In the next version of anndata (0.9) conversion will not be automatic. Pass dtype explicitly to avoid this warning. Pass `AnnData(X, dtype=X.dtype, ...)` to get the future behavour.\n",
            "\n",
            "/usr/local/lib/python3.8/dist-packages/scanpy/plotting/_tools/scatterplots.py:392: UserWarning:\n",
            "\n",
            "No data for colormapping provided via 'c'. Parameters 'cmap' will be ignored\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: You’re trying to run this on 13431 dimensions of `.X`, if you really want this, set `use_rep='X'`.\n",
            "         Falling back to preprocessing with `sc.pp.pca` and default params.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/anndata/_core/anndata.py:1785: FutureWarning:\n",
            "\n",
            "X.dtype being converted to np.float32 from float64. In the next version of anndata (0.9) conversion will not be automatic. Pass dtype explicitly to avoid this warning. Pass `AnnData(X, dtype=X.dtype, ...)` to get the future behavour.\n",
            "\n",
            "/usr/local/lib/python3.8/dist-packages/scanpy/plotting/_tools/scatterplots.py:392: UserWarning:\n",
            "\n",
            "No data for colormapping provided via 'c'. Parameters 'cmap' will be ignored\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: You’re trying to run this on 13431 dimensions of `.X`, if you really want this, set `use_rep='X'`.\n",
            "         Falling back to preprocessing with `sc.pp.pca` and default params.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/anndata/_core/anndata.py:1785: FutureWarning:\n",
            "\n",
            "X.dtype being converted to np.float32 from float64. In the next version of anndata (0.9) conversion will not be automatic. Pass dtype explicitly to avoid this warning. Pass `AnnData(X, dtype=X.dtype, ...)` to get the future behavour.\n",
            "\n",
            "/usr/local/lib/python3.8/dist-packages/scanpy/plotting/_tools/scatterplots.py:392: UserWarning:\n",
            "\n",
            "No data for colormapping provided via 'c'. Parameters 'cmap' will be ignored\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: You’re trying to run this on 13431 dimensions of `.X`, if you really want this, set `use_rep='X'`.\n",
            "         Falling back to preprocessing with `sc.pp.pca` and default params.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/anndata/_core/anndata.py:1785: FutureWarning:\n",
            "\n",
            "X.dtype being converted to np.float32 from float64. In the next version of anndata (0.9) conversion will not be automatic. Pass dtype explicitly to avoid this warning. Pass `AnnData(X, dtype=X.dtype, ...)` to get the future behavour.\n",
            "\n",
            "/usr/local/lib/python3.8/dist-packages/scanpy/plotting/_tools/scatterplots.py:392: UserWarning:\n",
            "\n",
            "No data for colormapping provided via 'c'. Parameters 'cmap' will be ignored\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: You’re trying to run this on 13431 dimensions of `.X`, if you really want this, set `use_rep='X'`.\n",
            "         Falling back to preprocessing with `sc.pp.pca` and default params.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/anndata/_core/anndata.py:1785: FutureWarning:\n",
            "\n",
            "X.dtype being converted to np.float32 from float64. In the next version of anndata (0.9) conversion will not be automatic. Pass dtype explicitly to avoid this warning. Pass `AnnData(X, dtype=X.dtype, ...)` to get the future behavour.\n",
            "\n",
            "/usr/local/lib/python3.8/dist-packages/scanpy/plotting/_tools/scatterplots.py:392: UserWarning:\n",
            "\n",
            "No data for colormapping provided via 'c'. Parameters 'cmap' will be ignored\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: You’re trying to run this on 13431 dimensions of `.X`, if you really want this, set `use_rep='X'`.\n",
            "         Falling back to preprocessing with `sc.pp.pca` and default params.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/anndata/_core/anndata.py:1785: FutureWarning:\n",
            "\n",
            "X.dtype being converted to np.float32 from float64. In the next version of anndata (0.9) conversion will not be automatic. Pass dtype explicitly to avoid this warning. Pass `AnnData(X, dtype=X.dtype, ...)` to get the future behavour.\n",
            "\n",
            "/usr/local/lib/python3.8/dist-packages/scanpy/plotting/_tools/scatterplots.py:392: UserWarning:\n",
            "\n",
            "No data for colormapping provided via 'c'. Parameters 'cmap' will be ignored\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: You’re trying to run this on 13431 dimensions of `.X`, if you really want this, set `use_rep='X'`.\n",
            "         Falling back to preprocessing with `sc.pp.pca` and default params.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/anndata/_core/anndata.py:1785: FutureWarning:\n",
            "\n",
            "X.dtype being converted to np.float32 from float64. In the next version of anndata (0.9) conversion will not be automatic. Pass dtype explicitly to avoid this warning. Pass `AnnData(X, dtype=X.dtype, ...)` to get the future behavour.\n",
            "\n",
            "/usr/local/lib/python3.8/dist-packages/scanpy/plotting/_tools/scatterplots.py:392: UserWarning:\n",
            "\n",
            "No data for colormapping provided via 'c'. Parameters 'cmap' will be ignored\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: You’re trying to run this on 13431 dimensions of `.X`, if you really want this, set `use_rep='X'`.\n",
            "         Falling back to preprocessing with `sc.pp.pca` and default params.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/anndata/_core/anndata.py:1785: FutureWarning:\n",
            "\n",
            "X.dtype being converted to np.float32 from float64. In the next version of anndata (0.9) conversion will not be automatic. Pass dtype explicitly to avoid this warning. Pass `AnnData(X, dtype=X.dtype, ...)` to get the future behavour.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "train_func(config=config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QJSgmoQF8N82"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3.9.13 ('scINTEGRATION')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "1d226e3599a48bcd2e3e064e4b49e64b5c23bb1e3c85e4572c7816e0051bede7"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}