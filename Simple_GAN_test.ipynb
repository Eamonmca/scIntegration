{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "yKRnz3qrnNXo",
        "outputId": "76f9af12-7b82-4ef4-de52-9107ab4e5e16"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: scanpy in /usr/local/lib/python3.8/dist-packages (1.9.1)\n",
            "Requirement already satisfied: anndata>=0.7.4 in /usr/local/lib/python3.8/dist-packages (from scanpy) (0.8.0)\n",
            "Requirement already satisfied: patsy in /usr/local/lib/python3.8/dist-packages (from scanpy) (0.5.3)\n",
            "Collecting matplotlib>=3.4\n",
            "  Using cached matplotlib-3.6.2-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (9.4 MB)\n",
            "Requirement already satisfied: networkx>=2.3 in /usr/local/lib/python3.8/dist-packages (from scanpy) (2.6.3)\n",
            "Requirement already satisfied: natsort in /usr/local/lib/python3.8/dist-packages (from scanpy) (5.5.0)\n",
            "Requirement already satisfied: numba>=0.41.0 in /usr/local/lib/python3.8/dist-packages (from scanpy) (0.56.4)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.8/dist-packages (from scanpy) (1.21.6)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from scanpy) (1.2.0)\n",
            "Requirement already satisfied: scipy>=1.4 in /usr/local/lib/python3.8/dist-packages (from scanpy) (1.7.3)\n",
            "Requirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.8/dist-packages (from scanpy) (1.0.2)\n",
            "Requirement already satisfied: session-info in /usr/local/lib/python3.8/dist-packages (from scanpy) (1.0.0)\n",
            "Requirement already satisfied: h5py>=3 in /usr/local/lib/python3.8/dist-packages (from scanpy) (3.1.0)\n",
            "Requirement already satisfied: umap-learn>=0.3.10 in /usr/local/lib/python3.8/dist-packages (from scanpy) (0.5.3)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.8/dist-packages (from scanpy) (0.11.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from scanpy) (4.64.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from scanpy) (21.3)\n",
            "Requirement already satisfied: pandas>=1.0 in /usr/local/lib/python3.8/dist-packages (from scanpy) (1.3.5)\n",
            "Requirement already satisfied: statsmodels>=0.10.0rc2 in /usr/local/lib/python3.8/dist-packages (from scanpy) (0.12.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.4->scanpy) (1.4.4)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.4->scanpy) (4.38.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.4->scanpy) (1.0.6)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.4->scanpy) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.4->scanpy) (2.8.2)\n",
            "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.4->scanpy) (3.0.9)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.4->scanpy) (7.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from numba>=0.41.0->scanpy) (57.4.0)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.8/dist-packages (from numba>=0.41.0->scanpy) (0.39.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.8/dist-packages (from numba>=0.41.0->scanpy) (4.13.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=1.0->scanpy) (2022.6)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7->matplotlib>=3.4->scanpy) (1.15.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.22->scanpy) (3.1.0)\n",
            "Requirement already satisfied: pynndescent>=0.5 in /usr/local/lib/python3.8/dist-packages (from umap-learn>=0.3.10->scanpy) (0.5.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata->numba>=0.41.0->scanpy) (3.10.0)\n",
            "Requirement already satisfied: stdlib-list in /usr/local/lib/python3.8/dist-packages (from session-info->scanpy) (0.8.0)\n",
            "Installing collected packages: matplotlib\n",
            "  Attempting uninstall: matplotlib\n",
            "    Found existing installation: matplotlib 3.1.3\n",
            "    Uninstalling matplotlib-3.1.3:\n",
            "      Successfully uninstalled matplotlib-3.1.3\n",
            "Successfully installed matplotlib-3.6.2\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "matplotlib",
                  "mpl_toolkits"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Usage:   \n",
            "  /usr/bin/python3 -m pip uninstall [options] <package> ...\n",
            "  /usr/bin/python3 -m pip uninstall [options] -r <requirements file> ...\n",
            "\n",
            "no such option: -Y\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting matplotlib==3.1.3\n",
            "  Using cached matplotlib-3.1.3-cp38-cp38-manylinux1_x86_64.whl (13.1 MB)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.8/dist-packages (from matplotlib==3.1.3) (1.21.6)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib==3.1.3) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib==3.1.3) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib==3.1.3) (3.0.9)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib==3.1.3) (1.4.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.1->matplotlib==3.1.3) (1.15.0)\n",
            "Installing collected packages: matplotlib\n",
            "  Attempting uninstall: matplotlib\n",
            "    Found existing installation: matplotlib 3.6.2\n",
            "    Uninstalling matplotlib-3.6.2:\n",
            "      Successfully uninstalled matplotlib-3.6.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "scanpy 1.9.1 requires matplotlib>=3.4, but you have matplotlib 3.1.3 which is incompatible.\u001b[0m\n",
            "Successfully installed matplotlib-3.1.3\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "matplotlib",
                  "mpl_toolkits"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.8/dist-packages (0.13.5)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (2.23.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from wandb) (57.4.0)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (1.9.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied: pathtools in /usr/local/lib/python3.8/dist-packages (from wandb) (0.1.2)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: protobuf!=4.0.*,!=4.21.0,<5,>=3.12.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (3.19.6)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.8/dist-packages (from wandb) (1.3.2)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (1.15.0)\n",
            "Requirement already satisfied: shortuuid>=0.5.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (1.0.11)\n",
            "Requirement already satisfied: GitPython>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (3.1.29)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.8/dist-packages (from wandb) (6.0)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (2.3)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.8/dist-packages (from GitPython>=1.0.0->wandb) (4.0.10)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.8/dist-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (5.0.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.0.0->wandb) (2022.9.24)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n"
          ]
        }
      ],
      "source": [
        " !pip install scanpy\n",
        " %matplotlib inline\n",
        "!python -m pip uninstall matplotlib -Y\n",
        "!pip install matplotlib==3.1.3\n",
        "!pip install wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "andwqcpJmgZO",
        "outputId": "960b9a05-a0b7-4e28-ad28-54eda1ba8272"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x7fd3b1f49e80>"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "from torch import nn, optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import itertools\n",
        "import scanpy as sc\n",
        "import plotly.express as px\n",
        "import plotly.io as pio\n",
        "import sklearn.preprocessing\n",
        "import sklearn.model_selection\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "torch.autograd.set_detect_anomaly(True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uAEfebOCnMkw"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "fyeiaHCwmgZQ"
      },
      "outputs": [],
      "source": [
        "import platform\n",
        "\n",
        "def get_device_and_gmount():\n",
        "    # Get the operating system and version\n",
        "    os = platform.system()\n",
        "    version = platform.release()\n",
        "\n",
        "    # Get the machine's architecture\n",
        "    arch = platform.machine()\n",
        "\n",
        "    # Set the default renderer based on the operating system\n",
        "    if os == 'Darwin':\n",
        "        pio.renderers.default = 'notebook'\n",
        "        print(\"Using Apple MPS on Macbook Pro\")\n",
        "    \n",
        "    elif os == 'Linux':\n",
        "        pio.renderers.default = 'colab'\n",
        "        print(\"Using Colab on Linux\")\n",
        "\n",
        "    # Set the device based on the machine's architecture\n",
        "    if arch == 'x86_64':\n",
        "        device = torch.device('mps') if os == 'Darwin' else torch.device('cuda')\n",
        "        gmount = True if os == 'Linux' else False\n",
        "    else:\n",
        "        device = torch.device('cpu')\n",
        "        gmount = False\n",
        "\n",
        "    print(\"Using device:\", device)\n",
        "    \n",
        "    return device, gmount\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Jn5HT1RmgZR",
        "outputId": "a3ccf1c7-3240-4148-a7a6-bfca64ee5c99"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using Apple MPS on Macbook Pro\n",
            "Using device: mps\n"
          ]
        }
      ],
      "source": [
        "device, gmount = get_device_and_gmount()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pFszDCKQmgZR",
        "outputId": "273093ff-debf-479a-906b-7023de715803"
      },
      "outputs": [],
      "source": [
        "if gmount:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    scdata = sc.read_h5ad(\"/content/drive/MyDrive/scintegration/GEX.h5ad\")\n",
        "else:\n",
        "    scdata = sc.read_h5ad(\"/Users/eamonmcandrew/Desktop/Single_cell_integration/Data/Multi-ome/GEX.h5ad\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "l8nIHsqGmgZR"
      },
      "outputs": [],
      "source": [
        "def stratified_split(data, test_size, random_state, split_criteria):\n",
        "    \"\"\"\n",
        "    Splits the data into train and test sets stratified by the batch column\n",
        "    \"\"\"\n",
        "    train = []\n",
        "    test = []\n",
        "    for batch in data.obs[split_criteria].unique():\n",
        "        batch_data = data[data.obs[split_criteria] == batch]\n",
        "        batch_train, batch_test = sklearn.model_selection.train_test_split(batch_data, test_size=test_size, random_state=random_state)\n",
        "        batch_train, batch_test = list(batch_train.obs.index), list(batch_test.obs.index)\n",
        "        train.extend(batch_train)\n",
        "        test.extend(batch_test)\n",
        "        \n",
        "    return train, test\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "re9X5gmLmgZS",
        "outputId": "60367a20-e914-47b8-b932-5aeb67f41629"
      },
      "outputs": [],
      "source": [
        "if gmount == True:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    scdata = sc.read_h5ad(\"/content/drive/MyDrive/scintegration/GEX.h5ad\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cT6PVLo8mgZS",
        "outputId": "4cdbf023-a9b3-4d5e-f1d3-38645f671eb0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33meamomc\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import wandb\n",
        "wandb.login()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "gcP6QkdbmgZS"
      },
      "outputs": [],
      "source": [
        "class GEX_Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, data, scaler=None, cat_var=None, label_encoder=None):\n",
        "        self.data = data\n",
        "        self.values = np.asarray(data.X.todense())\n",
        "        self.cat_var = cat_var\n",
        "\n",
        "        label_encoder_functions = {\n",
        "            \"numeric\": lambda: torch.tensor(sklearn.preprocessing.LabelEncoder().fit_transform(self.data.obs[self.cat_var]), dtype=torch.long),\n",
        "            \"range_map\": lambda: sklearn.preprocessing.LabelEncoder().fit_transform(self.data.obs[self.cat_var]).reshape(-1, 1),\n",
        "            \"one_hot\": lambda: sklearn.preprocessing.OneHotEncoder().fit_transform(sklearn.preprocessing.LabelEncoder().fit_transform(self.data.obs[self.cat_var]).reshape(-1, 1)).toarray()\n",
        "        }\n",
        "\n",
        "        if label_encoder in label_encoder_functions:\n",
        "            cat_var_data = label_encoder_functions[label_encoder]()\n",
        "            if label_encoder == \"range_map\":\n",
        "                cat_var_data = torch.tensor(sklearn.preprocessing.MinMaxScaler().fit_transform(cat_var_data), dtype=torch.float32)\n",
        "            elif label_encoder == \"one_hot\":\n",
        "                cat_var_data = torch.tensor(cat_var_data, dtype=torch.float32)\n",
        "        else:\n",
        "            cat_var_data = None\n",
        "        self.cat_var_data = cat_var_data\n",
        "\n",
        "        scaler_functions = {\n",
        "            \"Standard\": lambda: sklearn.preprocessing.StandardScaler().fit_transform(self.values),\n",
        "            \"MinMax\": lambda: sklearn.preprocessing.MinMaxScaler().fit_transform(self.values)\n",
        "        }\n",
        "\n",
        "        if scaler in scaler_functions:\n",
        "            self.scaled_values = torch.tensor(scaler_functions[scaler](), dtype=torch.float32)\n",
        "        else:\n",
        "            self.scaled_values = torch.tensor(self.values, dtype=torch.float32)\n",
        "\n",
        "    @property\n",
        "    def n_features(self):\n",
        "        return self.values.shape[1]\n",
        "\n",
        "    @property\n",
        "    def n_catagories(self):\n",
        "        return self.cat_var_data.shape[1] if self.cat_var_data is not None else 0\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.scaled_values[idx], self.cat_var_data[idx]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "cANaD_q4-XD8"
      },
      "outputs": [],
      "source": [
        "def stratified_split(data, test_size, random_state, split_criteria):\n",
        "    \"\"\"\n",
        "    Splits the data into train and test sets stratified by the batch column\n",
        "    \"\"\"\n",
        "    train = []\n",
        "    test = []\n",
        "    for batch in data.obs[split_criteria].unique():\n",
        "        batch_data = data[data.obs[split_criteria] == batch]\n",
        "        batch_train, batch_test = sklearn.model_selection.train_test_split(batch_data, test_size=test_size, random_state=random_state)\n",
        "        batch_train, batch_test = list(batch_train.obs.index), list(batch_test.obs.index)\n",
        "        train.extend(batch_train)\n",
        "        test.extend(batch_test)\n",
        "        \n",
        "    return train, test\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "fOkl1P4wmgZT"
      },
      "outputs": [],
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self, input_size, dropout, hidden_sizes, output_size, use_batch_norm):\n",
        "        super(Generator, self).__init__()\n",
        "\n",
        "        self.use_batch_norm = use_batch_norm\n",
        "\n",
        "        # Create a list of Linear layers with the specified hidden sizes\n",
        "        self.hidden_layers = [nn.Linear(input_size, hidden_size) for hidden_size in hidden_sizes]\n",
        "        # Create a BatchNorm1d layer for each hidden layer if use_batch_norm is True\n",
        "        self.batch_norm_layers = [nn.BatchNorm1d(hidden_size) for hidden_size in hidden_sizes] if use_batch_norm else []\n",
        "        # Create a Dropout layer for each hidden layer\n",
        "        self.dropout_layers = [nn.Dropout(dropout) for _ in hidden_sizes]\n",
        "        # Create a Linear layer for the output\n",
        "        self.output_layer = nn.Linear(hidden_sizes[-1], output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Loop through the hidden layers\n",
        "        for hidden_layer, batch_norm_layer, dropout_layer in zip(self.hidden_layers, self.batch_norm_layers, self.dropout_layers):\n",
        "            # Apply the hidden layer, batch norm layer (if use_batch_norm is True), and dropout layer\n",
        "            x = hidden_layer(x)\n",
        "            if self.use_batch_norm:\n",
        "                x = batch_norm_layer(x)\n",
        "            x = F.relu(x)\n",
        "            x = dropout_layer(x)\n",
        "        # Apply the output layer and return the output\n",
        "        x = self.output_layer(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Generator(\n",
              "  (output_layer): Linear(in_features=20, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Generator(10, 0.5, [20, 20], 10, True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "yD-8i06-mgZT"
      },
      "outputs": [],
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, input_size, dropout, hidden_sizes, output_size, use_batch_norm):\n",
        "        super(Discriminator, self).__init__()\n",
        "\n",
        "        self.use_batch_norm = use_batch_norm\n",
        "\n",
        "        # Create a list of Linear layers with the specified hidden sizes\n",
        "        self.hidden_layers = [nn.Linear(input_size, hidden_size) for hidden_size in hidden_sizes]\n",
        "        # Create a BatchNorm1d layer for each hidden layer if use_batch_norm is True\n",
        "        self.batch_norm_layers = [nn.BatchNorm1d(hidden_size) for hidden_size in hidden_sizes] if use_batch_norm else []\n",
        "        # Create a Dropout layer for each hidden layer\n",
        "        self.dropout_layers = [nn.Dropout(dropout) for _ in hidden_sizes]\n",
        "        # Create a Linear layer for the output\n",
        "        self.output_layer = nn.Linear(hidden_sizes[-1], output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Loop through the hidden layers\n",
        "        for hidden_layer, batch_norm_layer, dropout_layer in zip(self.hidden_layers, self.batch_norm_layers, self.dropout_layers):\n",
        "            # Apply the hidden layer, batch norm layer (if use_batch_norm is True), and dropout layer\n",
        "            x = hidden_layer(x)\n",
        "            if self.use_batch_norm:\n",
        "                x = batch_norm_layer(x)\n",
        "            x = F.relu(x)\n",
        "            x = dropout_layer(x)\n",
        "        # Apply the output layer and return the output\n",
        "        x = self.output_layer(x)\n",
        "        return torch.sigmoid(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "DDKHovIomgZT"
      },
      "outputs": [],
      "source": [
        "# Define the GAN model\n",
        "class GAN(nn.Module):\n",
        "    def __init__(self, generator, discriminator):\n",
        "        super().__init__()\n",
        "        self.generator = generator\n",
        "        self.discriminator = discriminator\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.discriminator(self.generator(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "HoK2bQef5Tkg"
      },
      "outputs": [],
      "source": [
        "def generate_fake_data_and_combine(original_data, generator, z_dim, batch_size):\n",
        "    generator.eval()\n",
        "    noise = torch.randn(batch_size, z_dim)\n",
        "    fake_data = generator(noise)\n",
        "    fake_adata = sc.AnnData(fake_data.detach().numpy())\n",
        "\n",
        "    # add an index column to the fake data, starting at the number of real cells\n",
        "    fake_adata.var.index = scdata.var.index\n",
        "    adata = sc.AnnData.concatenate(original_data, fake_adata)\n",
        "    # add a \"group\" column to the observations DataFrame,\n",
        "    # with the value \"real\" for the real cells and \"fake\" for the fake cells\n",
        "    adata.obs['group'] = ['real' if i < len(scdata.obs) else 'fake' for i in range(len(adata.obs))]\n",
        "\n",
        "    # compute the UMAP of the combined data\n",
        "    sc.pp.neighbors(adata, n_neighbors=10)\n",
        "    sc.tl.umap(adata)\n",
        "\n",
        "    return adata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EuXO3mOtBQmG"
      },
      "outputs": [],
      "source": [
        "def plot_umap_by_group(adata):\n",
        "    plot = sc.pl.umap(adata, color='group', legend_loc='on data', show=False)\n",
        "    return plot "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "hfcdWwrz5hZo"
      },
      "outputs": [],
      "source": [
        "def train_gan_one_epoch(epoch, GEX_dataloader_train, gan, optimizer, criterion):\n",
        "    gan.train()\n",
        "    for iteration, (data, _) in enumerate(GEX_dataloader_train):\n",
        "        data = data.to(device)\n",
        "        # Generate fake data\n",
        "        noise = torch.randn(data.shape[0], z_dim)\n",
        "        noise = noise.to(device)\n",
        "        fake_data = gan.generator(noise)\n",
        "\n",
        "        # Train the discriminator\n",
        "        optimizer.zero_grad()\n",
        "        pred_real = gan.discriminator(data)\n",
        "        pred_fake = gan.discriminator(fake_data.detach())\n",
        "        loss_real = criterion(pred_real, torch.ones_like(pred_real))\n",
        "        loss_fake = criterion(pred_fake, torch.zeros_like(pred_fake))\n",
        "        loss_discriminator = (loss_real + loss_fake) / 2\n",
        "        loss_discriminator.backward()\n",
        "\n",
        "        # Train the generator\n",
        "        pred_fake = gan.discriminator(fake_data)\n",
        "        loss_generator = criterion(pred_fake, torch.ones_like(pred_fake))\n",
        "        loss_generator.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    return loss_discriminator.item(), loss_generator.item()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "config = wandb.config = {\n",
        "    \"lr\": 0.001,\n",
        "    \"batch_size\": 64,\n",
        "    \"epochs\": 60,\n",
        "    \"random_seed\": 42,\n",
        "    \"dropout_G\": 0,\n",
        "    \"dropout_D\": 0.2,\n",
        "    \"split_criteria\": \"cell_type\",\n",
        "    \"eval_size_percentage\": 0.1,\n",
        "    \"hidden_sizes_G\": [32, 64, 128],\n",
        "    \"hidden_sizes_D\": [32, 64, 128],\n",
        "    \"use_batch_norm_G\": True,\n",
        "    \"use_batch_norm_D\": True,\n",
        "    \"z_dim\": 100,\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "AhfFLgLZz0iY"
      },
      "outputs": [],
      "source": [
        "def train_func(config):\n",
        "    \n",
        "    if config is not None:\n",
        "        config = wandb.config\n",
        "        run = wandb.init(project=\"Single Cell Omics integration\", entity=\"scintegration\", config=config)\n",
        "    else:\n",
        "      run = wandb.init(project=\"Single Cell Omics integration\", entity=\"scintegration\")\n",
        "    \n",
        "\n",
        "    # Load the learning rate, batch size, epochs, random seed, dropout, and hidden size from the wandb configuration\n",
        "\n",
        "    lr = wandb.config.lr\n",
        "    batch_size = wandb.config.batch_size\n",
        "    epochs = wandb.config.epochs\n",
        "    random_seed = wandb.config.random_seed\n",
        "    dropout_G = wandb.config.dropout_G\n",
        "    dropout_D = wandb.config.dropout_D\n",
        "    split_criteria = wandb.config.split_criteria\n",
        "    eval_size_percentage = wandb.config.eval_size_percentage\n",
        "    hidden_sizes_G = wandb.config.hidden_sizes_G\n",
        "    hidden_sizes_D = wandb.config.hidden_sizes_D\n",
        "    use_batch_norm_G = wandb.config.use_batch_norm_G\n",
        "    use_batch_norm_D = wandb.config.use_batch_norm_D\n",
        "    z_dim = wandb.config.z_dim\n",
        "\n",
        "\n",
        "    train, test = stratified_split(scdata, 0.2, wandb.config.random_seed, split_criteria=split_criteria)\n",
        "    train_data = scdata[train]\n",
        "    test_data = scdata[test]\n",
        "\n",
        "\n",
        "    GEX_Dataset_train = GEX_Dataset(train_data, scaler=\"Standard\", cat_var=\"batch\", label_encoder=\"one_hot\")\n",
        "    GEX_dataloader_train = torch.utils.data.DataLoader(GEX_Dataset_train, batch_size = 128, shuffle = True)\n",
        "\n",
        "    GEX_Dataset_test = GEX_Dataset(test_data, scaler=\"Standard\", cat_var=\"batch\", label_encoder=\"one_hot\")\n",
        "    GEX_dataloader_test = torch.utils.data.DataLoader(GEX_Dataset_test, batch_size = 128, shuffle = True)\n",
        "\n",
        "    output_size = GEX_Dataset.n_features\n",
        "    target_size = GEX_Dataset.n_features\n",
        "\n",
        "    # Create instance of the generator and discriminator networks\n",
        "    generator = Generator(input_size=z_dim, hidden_sizes=hidden_sizes_G, output_size=target_size, use_batch_norm = use_batch_norm_G, dropout=dropout_G)\n",
        "\n",
        "    discriminator = Discriminator(input_size=target_size, hidden_sizes=hidden_sizes_D, output_size=1, use_batch_norm = use_batch_norm_D, dropout=dropout_D)\n",
        "\n",
        "    gan = GAN(generator, discriminator)\n",
        "    gan = gan.to(device)\n",
        "\n",
        "    loss_fn = nn.BCELoss()\n",
        "    optimizer = torch.optim.Adam(gan.parameters(), lr=lr)\n",
        "    \n",
        "    test_UMAP = sc.pl.umap(test_data, color=\"cell_type\", show=False)\n",
        "    wandb.log({\"Test UMAP\": wandb.Image(test_UMAP)})\n",
        "    # Create instance of the GAN model\n",
        "    gan = GAN(generator, discriminator)\n",
        "    gan.train()\n",
        "    for epoch in range(1,epochs):\n",
        "      D_loss_train, G_loss_train = train_gan_one_epoch(epoch, GEX_dataloader_train, gan, optimizer, loss_fn)\n",
        "      wandb.log({\"D_loss_train\": D_loss_train, \"G_loss_train\": G_loss_train})\n",
        "      if epoch % 3 == 0:\n",
        "        umap_data = generate_fake_data_and_combine(test_data, generator, z_dim, batch_size)\n",
        "        umap = plot_umap_by_group(umap_data)\n",
        "        wandb.log({\"UMAP\": umap})\n",
        "\n",
        "      \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "SCnHz64fuJkZ"
      },
      "outputs": [],
      "source": [
        "wandb."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YuSZVH6orpCV"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pIbJ6Xn5BF1t"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.13 ('scINTEGRATION')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "1d226e3599a48bcd2e3e064e4b49e64b5c23bb1e3c85e4572c7816e0051bede7"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
