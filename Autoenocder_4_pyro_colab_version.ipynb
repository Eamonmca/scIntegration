{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QDKfXqtIGlDr",
        "outputId": "b1f3f69a-9987-4425-f06e-7fffc79a8bee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pyro-ppl in /usr/local/lib/python3.7/dist-packages (1.8.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from pyro-ppl) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.7/dist-packages (from pyro-ppl) (1.21.6)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from pyro-ppl) (1.12.1+cu113)\n",
            "Requirement already satisfied: tqdm>=4.36 in /usr/local/lib/python3.7/dist-packages (from pyro-ppl) (4.64.1)\n",
            "Requirement already satisfied: pyro-api>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from pyro-ppl) (0.1.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.11.0->pyro-ppl) (4.1.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: scanpy in /usr/local/lib/python3.7/dist-packages (1.9.1)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.7/dist-packages (from scanpy) (1.21.6)\n",
            "Requirement already satisfied: umap-learn>=0.3.10 in /usr/local/lib/python3.7/dist-packages (from scanpy) (0.5.3)\n",
            "Requirement already satisfied: pandas>=1.0 in /usr/local/lib/python3.7/dist-packages (from scanpy) (1.3.5)\n",
            "Requirement already satisfied: statsmodels>=0.10.0rc2 in /usr/local/lib/python3.7/dist-packages (from scanpy) (0.12.2)\n",
            "Requirement already satisfied: scipy>=1.4 in /usr/local/lib/python3.7/dist-packages (from scanpy) (1.7.3)\n",
            "Requirement already satisfied: natsort in /usr/local/lib/python3.7/dist-packages (from scanpy) (5.5.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from scanpy) (4.64.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from scanpy) (1.2.0)\n",
            "Requirement already satisfied: session-info in /usr/local/lib/python3.7/dist-packages (from scanpy) (1.0.0)\n",
            "Requirement already satisfied: networkx>=2.3 in /usr/local/lib/python3.7/dist-packages (from scanpy) (2.6.3)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.7/dist-packages (from scanpy) (0.11.2)\n",
            "Requirement already satisfied: importlib_metadata>=0.7 in /usr/local/lib/python3.7/dist-packages (from scanpy) (4.13.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from scanpy) (21.3)\n",
            "Requirement already satisfied: numba>=0.41.0 in /usr/local/lib/python3.7/dist-packages (from scanpy) (0.56.3)\n",
            "Requirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.7/dist-packages (from scanpy) (1.0.2)\n",
            "Requirement already satisfied: h5py>=3 in /usr/local/lib/python3.7/dist-packages (from scanpy) (3.1.0)\n",
            "Requirement already satisfied: anndata>=0.7.4 in /usr/local/lib/python3.7/dist-packages (from scanpy) (0.8.0)\n",
            "Requirement already satisfied: matplotlib>=3.4 in /usr/local/lib/python3.7/dist-packages (from scanpy) (3.5.3)\n",
            "Requirement already satisfied: patsy in /usr/local/lib/python3.7/dist-packages (from scanpy) (0.5.3)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.7/dist-packages (from anndata>=0.7.4->scanpy) (4.1.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=3->scanpy) (1.5.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib_metadata>=0.7->scanpy) (3.10.0)\n",
            "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.4->scanpy) (3.0.9)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.4->scanpy) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.4->scanpy) (4.38.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.4->scanpy) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.4->scanpy) (1.4.4)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.4->scanpy) (7.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba>=0.41.0->scanpy) (57.4.0)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.7/dist-packages (from numba>=0.41.0->scanpy) (0.39.1)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.0->scanpy) (2022.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7->matplotlib>=3.4->scanpy) (1.15.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.22->scanpy) (3.1.0)\n",
            "Requirement already satisfied: pynndescent>=0.5 in /usr/local/lib/python3.7/dist-packages (from umap-learn>=0.3.10->scanpy) (0.5.8)\n",
            "Requirement already satisfied: stdlib-list in /usr/local/lib/python3.7/dist-packages (from session-info->scanpy) (0.8.0)\n"
          ]
        }
      ],
      "source": [
        " !pip3 install pyro-ppl \n",
        " !pip install scanpy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zp4nEi8aa3sN",
        "outputId": "cbd9efa4-0fae-4a82-f46b-8cb37d54c140"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from pyro.contrib.examples.util import MNIST\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import pyro\n",
        "import pyro.distributions as dist\n",
        "import pyro.contrib.examples.util  # patches torchvision\n",
        "from pyro.infer import SVI, Trace_ELBO\n",
        "from pyro.optim import Adam\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import itertools\n",
        "import scanpy as sc\n",
        "from pandas_profiling import ProfileReport\n",
        "import plotly.express as px\n",
        "import plotly.io as pio\n",
        "import sklearn.preprocessing\n"
      ],
      "metadata": {
        "id": "UfU7l9uWGqIC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "assert pyro.__version__.startswith('1.8.2')\n",
        "pyro.distributions.enable_validation(False)\n",
        "pyro.set_rng_seed(0)\n",
        "# Enable smoke test - run the notebook cells on CI.\n",
        "smoke_test = 'CI' in os.environ"
      ],
      "metadata": {
        "id": "9ZNTS2vEGqnE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import platform\n",
        "platform.platform()\n",
        "if torch.cuda.is_available:\n",
        "     device = torch.device(\"cuda\")"
      ],
      "metadata": {
        "id": "BJvwUpf6H2EQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.is_available()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JRgXgSbhHr_q",
        "outputId": "20e3bd80-9997-4443-df0c-a4a4d9ca3cd2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scdata = sc.read_h5ad(\"/content/drive/MyDrive/scintegration/GEX.h5ad\")\n"
      ],
      "metadata": {
        "id": "zmFK24ZjbUJi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GEX_Dataset(torch.utils.data.Dataset):\n",
        "    \n",
        "      def __init__(self, data,  scaler = None, cat_var = None):\n",
        "          \n",
        "            self.data = data\n",
        "            \n",
        "            # we need to work with the dense matrix\n",
        "            self.values = data.X.todense()\n",
        "            \n",
        "            self.cat_var = cat_var\n",
        "            \n",
        "            # numerically encode the labels\n",
        "            self.cat_var_data =  torch.tensor(sklearn.preprocessing.LabelEncoder().fit_transform(self.data.obs[self.cat_var]))\n",
        "            \n",
        "            # scale the data according to user inpt to scaler argument\n",
        "            if scaler == \"Standard\":\n",
        "                self.scaled_values = torch.tensor(sklearn.preprocessing.StandardScaler().fit_transform(self.values))\n",
        "            elif scaler == \"MinMax\":\n",
        "                self.scaled_values = torch.tensor(sklearn.preprocessing.MinMaxScaler().fit_transform(self.values))\n",
        "            else:\n",
        "                self.scaled_values = torch.tensor(self.values)\n",
        "                \n",
        "    #   return the number of genes when called \n",
        "             \n",
        "      @property\n",
        "      def n_features(self):\n",
        "          return self.values.shape[1]\n",
        "          \n",
        "          \n",
        "    #  A dataset class needs the following two methods to work with the dataloader class     \n",
        "          \n",
        "    #   return the number of cells when called\n",
        "      def __len__(self):\n",
        "          return len(self.data)\n",
        "    \n",
        "    #  return an individual cell and its label when called\n",
        "      def __getitem__(self, idx):\n",
        "           return self.scaled_values[idx], self.cat_var_data[idx]\n"
      ],
      "metadata": {
        "id": "tkM9Byb7IBAx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GEX_Dataset = GEX_Dataset(scdata, scaler = \"Standard\", cat_var = \"batch\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1RIJ8J3PIUxX",
        "outputId": "f0cb3a83-d90f-46ad-8d8b-442311a605e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py:598: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py:598: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
            "  FutureWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_features = GEX_Dataset.n_features\n",
        "batch_size =128"
      ],
      "metadata": {
        "id": "5LYBfZxKIVmE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(GEX_Dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(GEX_Dataset, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "ZFM68HuHIa8D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_features = GEX_Dataset.n_features"
      ],
      "metadata": {
        "id": "s-ZA9KeiIc8d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, z_dim, hidden_size, output_size):\n",
        "        # call the init method of the nn.Module class\n",
        "        super().__init__()\n",
        "        \n",
        "        # setup the two linear transformations used\n",
        "        self.dfc1 = nn.Linear(z_dim, hidden_size)\n",
        "        self.dfc2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.dfc3 = nn.Linear(hidden_size, output_size)\n",
        "        # setup the non-linearities\n",
        "        self.Relu= nn.ReLU()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "    \n",
        "\n",
        "    def forward(self, z):\n",
        "        # define the forward computation on the latent z\n",
        "        # first compute the hidden units\n",
        "        hidden_1 = self.Relu(self.dfc1(z))\n",
        "        hidden_2 = self.Relu(self.dfc2(hidden_1))\n",
        "        # \n",
        "        # each is of size batch_size x n_features\n",
        "        output = self.sigmoid(self.dfc3(hidden_2))\n",
        "        return output"
      ],
      "metadata": {
        "id": "GLEN2xUsIgKd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, latent_size, hidden_size, input_size):\n",
        "        super().__init__()\n",
        "        # setup the linear transformations used\n",
        "        self.efc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.efc2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.efc3_mu = nn.Linear(hidden_size, latent_size)\n",
        "        self.efc3_sigma = nn.Linear(hidden_size, latent_size)\n",
        "        \n",
        "        # setup the non-linearities\n",
        "        self.Relu = nn.ReLU()\n",
        "        self.softplus = nn.Softplus()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # define the forward computation on the a cell x\n",
        "        \n",
        "        # then compute the hidden units\n",
        "        hidden_1 = self.Relu(self.efc1(x))\n",
        "        hidden_2 = self.softplus(self.efc2(hidden_1))\n",
        "        \n",
        "        # then return a mean vector and a (positive) square root covariance\n",
        "        # each of size batch_size x z_dim\n",
        "        z_loc = self.efc3_mu(hidden_2)\n",
        "        z_scale = torch.exp(self.efc3_sigma(hidden_2))\n",
        "        return z_loc, z_scale"
      ],
      "metadata": {
        "id": "tZ91Q20VIimO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define the guide (i.e. variational distribution) q(z|x)\n",
        "def guide(self, x):\n",
        "    # register PyTorch module `encoder` with Pyro\n",
        "    pyro.module(\"encoder\", self.encoder)\n",
        "    with pyro.plate(\"data\", x.shape[0]):\n",
        "        # use the encoder to get the parameters used to define q(z|x)\n",
        "        z_loc, z_scale = self.encoder(x)\n",
        "        # sample the latent code z\n",
        "        pyro.sample(\"latent\", dist.Normal(z_loc, z_scale).to_event(1))"
      ],
      "metadata": {
        "id": "RqpOXtcqIlMg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VAE(nn.Module):\n",
        "    # by default our latent space is 50-dimensional\n",
        "    # and we use 400 hidden units\n",
        "    def __init__(self, z_dim=50, hidden_dim=200, input_size = n_features, use_cuda=False):\n",
        "        super().__init__()\n",
        "        # create the encoder and decoder networks\n",
        "        self.encoder = Encoder(z_dim, hidden_dim, input_size)\n",
        "        self.decoder = Decoder(z_dim, hidden_dim, output_size=input_size)\n",
        "\n",
        "        if use_cuda:\n",
        "            # calling cuda() here will put all the parameters of\n",
        "            # the encoder and decoder networks into gpu memory\n",
        "            self.to(device)\n",
        "        self.use_mps = use_cuda\n",
        "        self.z_dim = z_dim\n",
        "\n",
        "    # define the model p(x|z)p(z)\n",
        "    def model(self, x):\n",
        "        # register PyTorch module `decoder` with Pyro\n",
        "        pyro.module(\"decoder\", self.decoder)\n",
        "        with pyro.plate(\"data\", x.shape[0]):\n",
        "            # setup hyperparameters for prior p(z)\n",
        "            z_loc = x.new_zeros(torch.Size((x.shape[0], self.z_dim)))\n",
        "            z_scale = x.new_ones(torch.Size((x.shape[0], self.z_dim)))\n",
        "            # sample from prior (value will be sampled by guide when computing the ELBO)\n",
        "            z = pyro.sample(\"latent\", dist.Normal(z_loc, z_scale).to_event(1))\n",
        "            # decode the latent code z\n",
        "            output = self.decoder(z)\n",
        "            # score against actual images\n",
        "            pyro.sample(\"obs\", dist.Bernoulli(output).to_event(1), obs=x.reshape(-1, n_features))\n",
        "\n",
        "\n",
        "    # define the guide (i.e. variational distribution) q(z|x)\n",
        "    def guide(self, x):\n",
        "        # register PyTorch module `encoder` with Pyro\n",
        "        pyro.module(\"encoder\", self.encoder)\n",
        "        with pyro.plate(\"data\", x.shape[0]):\n",
        "            # use the encoder to get the parameters used to define q(z|x)\n",
        "            z_loc, z_scale = self.encoder(x)\n",
        "            # sample the latent code z\n",
        "            pyro.sample(\"latent\", dist.Normal(z_loc, z_scale).to_event(1))\n",
        "\n",
        "    # define a helper function for reconstructing images\n",
        "    def reconstruct_img(self, x):\n",
        "        # encode image x\n",
        "        z_loc, z_scale = self.encoder(x)\n",
        "        # sample in latent space\n",
        "        z = dist.Normal(z_loc, z_scale).sample()\n",
        "        # decode the image (note we don't sample in image space)\n",
        "        output = self.decoder(z)\n",
        "        return output"
      ],
      "metadata": {
        "id": "hRzN00M3InMH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vae = VAE(use_cuda=True)"
      ],
      "metadata": {
        "id": "Z4zq5qWAIqEV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = Adam({\"lr\": 1.0e-3})\n"
      ],
      "metadata": {
        "id": "ofz3g_yLI0VI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "svi = SVI(vae.model, vae.guide, optimizer, loss=Trace_ELBO())"
      ],
      "metadata": {
        "id": "lxMVgpCAI2SM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(svi, train_loader, use_cuda=True):\n",
        "    # initialize loss accumulator\n",
        "    epoch_loss = 0.\n",
        "    # do a training epoch over each mini-batch x returned\n",
        "    # by the data loader\n",
        "    for x, _ in train_loader:\n",
        "        # if on GPU put mini-batch into CUDA memory\n",
        "        if use_cuda:\n",
        "            x = x.to(device)\n",
        "        # do ELBO gradient and accumulate loss\n",
        "        epoch_loss += svi.step(x)\n",
        "\n",
        "    # return epoch loss\n",
        "    normalizer_train = len(train_loader.dataset)\n",
        "    total_epoch_loss_train = epoch_loss / normalizer_train\n",
        "    return total_epoch_loss_train"
      ],
      "metadata": {
        "id": "0bUEXj_KI4Ho"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(svi, test_loader, use_cuda=True):\n",
        "    # initialize loss accumulator\n",
        "    test_loss = 0.\n",
        "    # compute the loss over the entire test set\n",
        "    for x, _ in test_loader:\n",
        "        # if on GPU put mini-batch into CUDA memory\n",
        "        if use_cuda:\n",
        "            x = x.cuda()\n",
        "        # compute ELBO estimate and accumulate loss\n",
        "        test_loss += svi.evaluate_loss(x)\n",
        "    normalizer_test = len(test_loader.dataset)\n",
        "    total_epoch_loss_test = test_loss / normalizer_test\n",
        "    return total_epoch_loss_test"
      ],
      "metadata": {
        "id": "2-Ssfd61I7oT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run options\n",
        "LEARNING_RATE = 1.0e-4\n",
        "USE_CUDA = True\n",
        "\n",
        "# Run only for a single iteration for testing\n",
        "NUM_EPOCHS = 1 if smoke_test else 10\n",
        "TEST_FREQUENCY = 5"
      ],
      "metadata": {
        "id": "xk-T9EX6I9sD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# clear param store\n",
        "pyro.clear_param_store()\n",
        "\n",
        "# setup the VAE\n",
        "vae = VAE(use_cuda=True)\n",
        "\n",
        "# setup the optimizer\n",
        "adam_args = {\"lr\": LEARNING_RATE}\n",
        "optimizer = Adam(adam_args)\n",
        "\n",
        "# setup the inference algorithm\n",
        "svi = SVI(vae.model, vae.guide, optimizer, loss=Trace_ELBO())\n",
        "\n",
        "train_elbo = []\n",
        "test_elbo = []\n",
        "# training loop\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    total_epoch_loss_train = train(svi, train_loader, use_cuda=True)\n",
        "    train_elbo.append(-total_epoch_loss_train)\n",
        "    print(\"[epoch %03d]  average training loss: %.4f\" % (epoch, total_epoch_loss_train))\n",
        "\n",
        "    if epoch % TEST_FREQUENCY == 0:\n",
        "        # report test diagnostics\n",
        "        total_epoch_loss_test = evaluate(svi, test_loader, use_cuda=True)\n",
        "        test_elbo.append(-total_epoch_loss_test)\n",
        "        print(\"[epoch %03d] average test loss: %.4f\" % (epoch, total_epoch_loss_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A_Hfq9rvJAMd",
        "outputId": "d3385854-2210-41f7-e1a6-3f00a8c1040e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[epoch 000]  average training loss: -3498.6810\n",
            "[epoch 000] average test loss: -5470.4510\n",
            "[epoch 001]  average training loss: -5959.9899\n",
            "[epoch 002]  average training loss: -8625.9631\n",
            "[epoch 003]  average training loss: -9630.4942\n",
            "[epoch 004]  average training loss: -10156.4808\n",
            "[epoch 005]  average training loss: -10504.4505\n",
            "[epoch 005] average test loss: -10694.1128\n",
            "[epoch 006]  average training loss: -10786.4320\n",
            "[epoch 007]  average training loss: -11022.7556\n",
            "[epoch 008]  average training loss: -11221.0258\n",
            "[epoch 009]  average training loss: -11377.7079\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vae.reconstruct_img()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        },
        "id": "mmSWNg6NRPTC",
        "outputId": "d3e7ee00-3102-4650-fd1f-198cc295ad05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-24a041b60537>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreconstruct_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: reconstruct_img() missing 1 required positional argument: 'x'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MGE6EO40RZ9N"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}