{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x7f966007cf70>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "import scanpy as sc\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "import sklearn.preprocessing\n",
    "import sklearn.model_selection\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Apple MPS on Macbook Pro\n"
     ]
    }
   ],
   "source": [
    "import platform\n",
    "if platform.platform() == 'macOS-10.16-x86_64-i386-64bit':\n",
    "    pio.renderers.default = 'notebook'\n",
    "    device = torch.device('mps')\n",
    "    print(\"Using Apple MPS on Macbook Pro\")\n",
    "    gmount = False\n",
    "    \n",
    "elif platform.platform() == 'Linux-5.10.133+-x86_64-with-Ubuntu-18.04-bionic':\n",
    "    pio.renderers.default = 'colab'\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        print(\"Using CUDA GPU on Colab\")\n",
    "        gmount = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "scdata = sc.read_h5ad(\"/Users/eamonmcandrew/Desktop/Single_cell_integration/Data/Multi-ome/GEX.h5ad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33meamomc\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = wandb.config = {\n",
    "  \"lr\" : 0.005818,\n",
    "  \"epochs\": 200,\n",
    "  \"batch_size\": 256,\n",
    "  \"dropout\": 0.2,\n",
    "  \"hidden_size\": 30,\n",
    "  \"random_seed\": 9000,\n",
    "  \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stratified_split(data, test_size, random_state, split_criteria):\n",
    "    \"\"\"\n",
    "    Splits the data into train and test sets stratified by the batch column\n",
    "    \"\"\"\n",
    "    train = []\n",
    "    test = []\n",
    "    for batch in data.obs[split_criteria].unique():\n",
    "        batch_data = data[data.obs[split_criteria] == batch]\n",
    "        batch_train, batch_test = sklearn.model_selection.train_test_split(batch_data, test_size=test_size, random_state=random_state)\n",
    "        batch_train, batch_test = list(batch_train.obs.index), list(batch_test.obs.index)\n",
    "        train.extend(batch_train)\n",
    "        test.extend(batch_test)\n",
    "        \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if gmount == True:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    path = '/content/drive/My Drive/Colab Notebooks/Experiments/' \n",
    "    scdata = sc.read_h5ad(\"/content/gdrive/MyDrive/scintegration/GEX.h5ad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use own weights and biases account by adding the Auth token when prompted, can also use key = 'offline' to use offline\n",
    "\n",
    "import wandb\n",
    "wandb.login()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GEX_Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, scaler=None, cat_var=None, label_encoder=None):\n",
    "        self.data = data\n",
    "        self.values = np.asarray(data.X.todense())\n",
    "        self.cat_var = cat_var\n",
    "\n",
    "        label_encoder_functions = {\n",
    "            \"numeric\": lambda: torch.tensor(sklearn.preprocessing.LabelEncoder().fit_transform(self.data.obs[self.cat_var]), dtype=torch.long),\n",
    "            \"range_map\": lambda: sklearn.preprocessing.LabelEncoder().fit_transform(self.data.obs[self.cat_var]).reshape(-1, 1),\n",
    "            \"one_hot\": lambda: sklearn.preprocessing.OneHotEncoder().fit_transform(sklearn.preprocessing.LabelEncoder().fit_transform(self.data.obs[self.cat_var]).reshape(-1, 1)).toarray()\n",
    "        }\n",
    "\n",
    "        if label_encoder in label_encoder_functions:\n",
    "            cat_var_data = label_encoder_functions[label_encoder]()\n",
    "            if label_encoder == \"range_map\":\n",
    "                cat_var_data = torch.tensor(sklearn.preprocessing.MinMaxScaler().fit_transform(cat_var_data), dtype=torch.float32)\n",
    "            elif label_encoder == \"one_hot\":\n",
    "                cat_var_data = torch.tensor(cat_var_data, dtype=torch.float32)\n",
    "        else:\n",
    "            cat_var_data = None\n",
    "        self.cat_var_data = cat_var_data\n",
    "\n",
    "        scaler_functions = {\n",
    "            \"Standard\": lambda: sklearn.preprocessing.StandardScaler().fit_transform(self.values),\n",
    "            \"MinMax\": lambda: sklearn.preprocessing.MinMaxScaler().fit_transform(self.values)\n",
    "        }\n",
    "\n",
    "        if scaler in scaler_functions:\n",
    "            self.scaled_values = torch.tensor(scaler_functions[scaler](), dtype=torch.float32)\n",
    "        else:\n",
    "            self.scaled_values = torch.tensor(self.values, dtype=torch.float32)\n",
    "\n",
    "    @property\n",
    "    def n_features(self):\n",
    "        return self.values.shape[1]\n",
    "\n",
    "    @property\n",
    "    def n_catagories(self):\n",
    "        return self.cat_var_data.shape[1] if self.cat_var_data is not None else 0\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.scaled_values[idx], self.cat_var_data[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class classifier(nn.Module):\n",
    "    def __init__(self, input_size, dropout, hidden_size, output_size):\n",
    "        super(classifier, self).__init__()\n",
    "        self.cfc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.cfc2 = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.cfc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.cfc2(x)\n",
    "        x = F.softmax(x, dim = 1)\n",
    "        return(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(epoch, GEX_dataloader_train, model, optimizer, criterion):\n",
    "    # Set the model to train mode\n",
    "    model.train()\n",
    "    \n",
    "    # Initialize lists to store the losses and accuracies for each batch\n",
    "    epoch_loss_list = []\n",
    "    epoch_accuracy_list = []\n",
    "    \n",
    "    # Iterate over the batches in the dataloader\n",
    "    for batch_idx, (data, target) in enumerate(GEX_dataloader_train):\n",
    "        # Move the data and target tensors to the specified device (GPU)\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        # Clear the gradients of all optimized parameters\n",
    "        optimizer.zero_grad()\n",
    "        # Feed the data through the model and get the output\n",
    "        output = model(data)\n",
    "        # Calculate the loss using the specified loss function\n",
    "        loss = criterion(output, target)\n",
    "        # Calculate the accuracy by comparing the model's predictions to the ground truth labels\n",
    "        accuracy = (output.argmax(1) == target.argmax(1)).type(torch.float).mean().item()\n",
    "        # Backpropagate the loss to update the model's parameters\n",
    "        loss.backward()\n",
    "        # Update the model's parameters using the optimizer\n",
    "        optimizer.step()\n",
    "        # Append the loss and accuracy for this batch to the corresponding lists\n",
    "        epoch_loss_list.append(loss.item())\n",
    "        epoch_accuracy_list.append(accuracy)\n",
    "        \n",
    "    # Calculate the mean loss and accuracy for the entire epoch\n",
    "    epoch_loss = np.mean(epoch_loss_list)\n",
    "    epoch_accuracy = np.mean(epoch_accuracy_list)\n",
    "    \n",
    "    # Return the epoch loss and accuracy\n",
    "    return epoch_loss, epoch_accuracy\n",
    "\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_one_epoch(epoch, GEX_Dataset_test, model, optimizer, criterion):\n",
    "    # Set the model to eval mode\n",
    "    model.eval()\n",
    "    # Tell PyTorch not to track gradients while evaluating the model\n",
    "    with torch.no_grad():\n",
    "        # Initialize lists to store the losses and accuracies for each batch\n",
    "        epoch_loss_list = []\n",
    "        epoch_accuracy_list = []\n",
    "        \n",
    "        # Iterate over the batches in the dataloader\n",
    "        for batch_idx, (data, target) in enumerate(GEX_Dataset_test):\n",
    "            # Move the data and target tensors to the specified device (GPU)\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            # Feed the data through the model and get the output\n",
    "            output = model(data)\n",
    "            # Calculate the loss using the specified loss function\n",
    "            loss = criterion(output, target)\n",
    "            # Calculate the accuracy by comparing the model's predictions to the ground truth labels\n",
    "            accuracy = (output.argmax(1) == target.argmax(1)).type(torch.float).mean().item()\n",
    "            # Append the loss and accuracy for this batch to the corresponding lists\n",
    "            epoch_loss_list.append(loss.item())\n",
    "            epoch_accuracy_list.append(accuracy)\n",
    "            # Log the batch loss and accuracy\n",
    "            \n",
    "            # Calculate and log the confusion matrix for this batch\n",
    "            # ground_truth_class_ids = target.argmax(1).cpu().numpy()\n",
    "            # predicted_class_ids = output.argmax(1).cpu().numpy()\n",
    "            # wandb.log({\"conf_mat\" : wandb.plot.confusion_matrix(probs=None, y_true=ground_truth_class_ids, preds=predicted_class_ids, class_names=scdata.obs[\"batch\"].unique())})\n",
    "        \n",
    "        # Calculate the mean loss and accuracy for the entire epoch\n",
    "        epoch_loss = np.mean(epoch_loss_list)\n",
    "        epoch_accuracy = np.mean(epoch_accuracy_list)\n",
    "        \n",
    "    # Return the epoch loss and accuracy\n",
    "    return epoch_loss, epoch_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_func(config):\n",
    "    # Initialize a new run in Weights & Biases (wandb)\n",
    "    run = wandb.init(project=\"Single Cell Omics integration\", entity=\"scintegration\", notes=\"Fine_tune_best_classifier\", config=config)\n",
    "\n",
    "    # Load the learning rate, batch size, epochs, random seed, dropout, and hidden size from the wandb configuration\n",
    "    lr = wandb.config.lr\n",
    "    batch_size = wandb.config.batch_size\n",
    "    epochs = wandb.config.epochs\n",
    "    random_seed = wandb.config.random_seed\n",
    "    dropout = wandb.config.dropout\n",
    "    hidden_size = wandb.config.hidden_size\n",
    "    \n",
    "    \n",
    "    # train test split\n",
    "    train, test = stratified_split(scdata, 0.2, wandb.config.random_seed, split_criteria='cell_type')\n",
    "    train_data = scdata[train]\n",
    "    test_data = scdata[test]\n",
    "    \n",
    "    # create datasets\n",
    "    GEX_Dataset_train = GEX_Dataset(train_data, scaler = \"Standard\", cat_var = \"batch\", label_encoder = \"one_hot\")\n",
    "    GEX_Dataset_test = GEX_Dataset(test_data, scaler = \"Standard\", cat_var = \"batch\", label_encoder = \"one_hot\")\n",
    "    \n",
    "    # Create dataloaders for the training and test datasets\n",
    "    GEX_dataloader_train = torch.utils.data.DataLoader(GEX_Dataset_train, batch_size = wandb.config.batch_size, shuffle = True)\n",
    "    GEX_dataloader_test = torch.utils.data.DataLoader(GEX_Dataset_test, batch_size = wandb.config.batch_size, shuffle = True)\n",
    "    \n",
    "    input_size = GEX_Dataset_train.n_features\n",
    "    output_size = GEX_Dataset_train.n_catagories\n",
    "    \n",
    "\n",
    "    # Instantiate the model, optimizer, and criterion outside the for loop\n",
    "    model = classifier(input_size=input_size, dropout=dropout, hidden_size=hidden_size, output_size=output_size)\n",
    "\n",
    "    # Move the model to the specified device (e.g. GPU)\n",
    "    model.to(device)\n",
    "\n",
    "    # Instantiate the Adam optimizer with the specified learning rate\n",
    "    optimizer = optim.Adam(model.parameters(), lr=wandb.config.lr)\n",
    "\n",
    "    # Instantiate the Cross Entropy loss function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        # Train the model for one epoch\n",
    "        train_loss, train_acc = train_one_epoch(epoch, GEX_dataloader_train, model, optimizer, criterion)\n",
    "\n",
    "        # Evaluate the model on the test dataset\n",
    "        val_loss, val_acc = evaluate_one_epoch(epoch, GEX_dataloader_test, model, optimizer, criterion)\n",
    "\n",
    "        # Log the epoch, train accuracy, train loss, validation accuracy, and validation loss to wandb\n",
    "        wandb.log({\n",
    "          'epoch': epoch, \n",
    "          'train_acc': train_acc,\n",
    "          'train_loss': train_loss, \n",
    "          'val_acc': val_acc, \n",
    "          'val_loss': val_loss\n",
    "        })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33meamomc\u001b[0m (\u001b[33mscintegration\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/eamonmcandrew/Desktop/Single_cell_integration/Github/scIntegration/wandb/run-20221202_111756-1ubjom4k</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/scintegration/Single%20Cell%20Omics%20integration/runs/1ubjom4k\" target=\"_blank\">noble-dust-86</a></strong> to <a href=\"https://wandb.ai/scintegration/Single%20Cell%20Omics%20integration\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_func()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "config2 = wandb.config = {\n",
    "  \"lr\" : 0.00361,\n",
    "  \"epochs\": 100,\n",
    "  \"batch_size\": 512,\n",
    "  \"dropout\": 0.2,\n",
    "  \"hidden_size\": 20,\n",
    "  \"random_seed\": 9000,\n",
    "  \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:1ubjom4k) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9cca98eb33346139f658f5437f299a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Train epoch accuracy</td><td>▁▅▆▆▆▆▆▇▇▇▆▇▇▇▇▆▇▇▇▇▇▇▇▇▇█▇██▆▇██▇▇▇█▇▇█</td></tr><tr><td>Train epoch loss</td><td>█▄▃▃▃▃▃▂▂▂▃▂▂▂▂▃▂▂▂▂▂▂▂▂▂▁▂▁▁▃▂▁▁▂▂▂▁▂▂▁</td></tr><tr><td>Val loss</td><td>▇█▇▅▇▆▇▁▆█▄▆▅▃▃▃▅▄▅▂▆▂▄▄▅▃▅▃▃▇▅▃▃▄▅▅▂▆▄▅</td></tr><tr><td>Validation epoch accuracy</td><td>▃▃▄▄▆▃▅▇▅▆▅▆▆▆▆▅▅▆▆▇▇▆▆▇▆▆▆▆█▁▇▆▆▅▆▆▇▇▆▇</td></tr><tr><td>Validation epoch loss</td><td>▆▆▅▅▃▆▄▂▄▃▄▃▃▃▃▄▄▃▃▂▂▃▃▂▃▃▃▃▁█▂▃▃▄▃▃▂▂▃▂</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train_acc</td><td>▁▅▆▆▆▆▆▇▇▇▆▇▇▇▇▆▇▇▇▇▇▇▇▇▇█▇██▆▇██▇▇▇█▇▇█</td></tr><tr><td>train_loss</td><td>█▄▃▃▃▃▃▂▂▂▃▂▂▂▂▃▂▂▂▂▂▂▂▂▂▁▂▁▁▃▂▁▁▂▂▂▁▂▂▁</td></tr><tr><td>val accuracy</td><td>▂▁▂▄▂▃▂█▃▁▅▃▄▆▆▆▄▅▄▇▃▇▅▅▄▆▄▆▆▂▄▆▆▅▄▄▇▃▅▄</td></tr><tr><td>val_acc</td><td>▃▃▄▄▆▃▅▇▅▆▅▆▆▆▆▅▅▆▆▇▇▆▆▇▆▆▆▆█▁▇▆▆▅▆▆▇▇▆▇</td></tr><tr><td>val_loss</td><td>▆▆▅▅▃▆▄▂▄▃▄▃▃▃▃▄▄▃▃▂▂▃▃▂▃▃▃▃▁█▂▃▃▄▃▃▂▂▃▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Train epoch accuracy</td><td>0.78836</td></tr><tr><td>Train epoch loss</td><td>1.90069</td></tr><tr><td>Val loss</td><td>1.84061</td></tr><tr><td>Validation epoch accuracy</td><td>0.77196</td></tr><tr><td>Validation epoch loss</td><td>1.91708</td></tr><tr><td>epoch</td><td>200</td></tr><tr><td>train_acc</td><td>0.78836</td></tr><tr><td>train_loss</td><td>1.90069</td></tr><tr><td>val accuracy</td><td>0.84848</td></tr><tr><td>val_acc</td><td>0.77196</td></tr><tr><td>val_loss</td><td>1.91708</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">noble-dust-86</strong>: <a href=\"https://wandb.ai/scintegration/Single%20Cell%20Omics%20integration/runs/1ubjom4k\" target=\"_blank\">https://wandb.ai/scintegration/Single%20Cell%20Omics%20integration/runs/1ubjom4k</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20221202_111756-1ubjom4k/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:1ubjom4k). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18f9894eae204522885edb8579f3266f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016721261116663298, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/eamonmcandrew/Desktop/Single_cell_integration/Github/scIntegration/wandb/run-20221202_115811-39zo5e67</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/scintegration/Single%20Cell%20Omics%20integration/runs/39zo5e67\" target=\"_blank\">olive-planet-87</a></strong> to <a href=\"https://wandb.ai/scintegration/Single%20Cell%20Omics%20integration\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_func(config2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "config3 = wandb.config = {\n",
    "  \"lr\" : 0.00361,\n",
    "  \"epochs\": 100,\n",
    "  \"batch_size\": 256,\n",
    "  \"dropout\": 0.2,\n",
    "  \"hidden_size\": 20,\n",
    "  \"random_seed\": 9000,\n",
    "  \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:39zo5e67) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c52e8de3e90c459c8ca70e600d11bdae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Train epoch accuracy</td><td>▁▆▇▇████████████████████████████████████</td></tr><tr><td>Train epoch loss</td><td>█▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Val loss</td><td>▇▇▅▆▅▄▃▅▄▅▆▄▄▅▅▅▅▅▅▃▅▄█▅▅▇▆▆▅▄▅▄▅▄▅▄▅▅▁▅</td></tr><tr><td>Validation epoch accuracy</td><td>▁▄█████▇███▇█████▇▇█▇█████████████████▇█</td></tr><tr><td>Validation epoch loss</td><td>█▅▁▁▁▁▁▂▁▁▁▂▁▁▁▁▁▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train_acc</td><td>▁▆▇▇████████████████████████████████████</td></tr><tr><td>train_loss</td><td>█▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val accuracy</td><td>▂▂▄▃▄▄▅▄▄▄▃▄▅▃▄▄▄▄▄▆▄▄▁▄▄▂▃▃▃▄▄▄▄▄▄▅▄▄█▃</td></tr><tr><td>val_acc</td><td>▁▄█████▇███▇█████▇▇█▇█████████████████▇█</td></tr><tr><td>val_loss</td><td>█▅▁▁▁▁▁▂▁▁▁▂▁▁▁▁▁▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Train epoch accuracy</td><td>0.88941</td></tr><tr><td>Train epoch loss</td><td>1.79958</td></tr><tr><td>Val loss</td><td>1.90121</td></tr><tr><td>Validation epoch accuracy</td><td>0.84713</td></tr><tr><td>Validation epoch loss</td><td>1.84166</td></tr><tr><td>epoch</td><td>100</td></tr><tr><td>train_acc</td><td>0.88941</td></tr><tr><td>train_loss</td><td>1.79958</td></tr><tr><td>val accuracy</td><td>0.78788</td></tr><tr><td>val_acc</td><td>0.84713</td></tr><tr><td>val_loss</td><td>1.84166</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">olive-planet-87</strong>: <a href=\"https://wandb.ai/scintegration/Single%20Cell%20Omics%20integration/runs/39zo5e67\" target=\"_blank\">https://wandb.ai/scintegration/Single%20Cell%20Omics%20integration/runs/39zo5e67</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20221202_115811-39zo5e67/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:39zo5e67). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dfc9ba597a34a21a498bcf1be194aae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016670984033339664, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/eamonmcandrew/Desktop/Single_cell_integration/Github/scIntegration/wandb/run-20221202_120433-3upq7s5r</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/scintegration/Single%20Cell%20Omics%20integration/runs/3upq7s5r\" target=\"_blank\">golden-voice-88</a></strong> to <a href=\"https://wandb.ai/scintegration/Single%20Cell%20Omics%20integration\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_func(config3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('scINTEGRATION')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1d226e3599a48bcd2e3e064e4b49e64b5c23bb1e3c85e4572c7816e0051bede7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
